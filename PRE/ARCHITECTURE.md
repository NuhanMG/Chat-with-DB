# COMPREHENSIVE SYSTEM ARCHITECTURE DOCUMENTATION
# Conversational Data Analytics System with Multi-Turn Context & Visualization

**Version:** 2.0  
**Last Updated:** November 21, 2025  
**Target Audience:** Developers, System Administrators, Technical Architects

---

## TABLE OF CONTENTS

1. [System Overview](#system-overview)
2. [High-Level Architecture](#high-level-architecture)
3. [Core Components Deep Dive](#core-components-deep-dive)
4. [Data Flow & Workflows](#data-flow--workflows)
5. [Conversation Management System](#conversation-management-system)
6. [Visualization Pipeline](#visualization-pipeline)
7. [Database & Vector Storage](#database--vector-storage)
8. [Class & Function Reference](#class--function-reference)
9. [Configuration & Parameters](#configuration--parameters)
10. [Data Structures & Examples](#data-structures--examples)
11. [Error Handling & Recovery](#error-handling--recovery)
12. [Performance & Optimization](#performance--optimization)
13. [Complete Deployment Guide - From Zero to Production](#complete-deployment-guide---from-zero-to-production)
14. [End-to-End User Guide - Getting Final Answers](#end-to-end-user-guide---getting-final-answers)

---

## SYSTEM OVERVIEW

### Purpose
This system is a **conversational data analytics platform** that enables natural language querying of SQLite databases with:
- **Multi-turn conversation support** with full context retention
- **Intelligent SQL generation** using LLM (Large Language Models)
- **Automatic visualization recommendation** and generation
- **Vector database-powered metadata retrieval** for semantic search
- **Persistent conversation history** with ability to restore sessions
- **Interactive web interface** using Gradio

### Key Capabilities
1. **Natural Language to SQL**: Convert user questions into executable SQL queries
2. **Contextual Awareness**: Remember previous queries and reference them in follow-up questions
3. **Smart Visualization**: Automatically detect when to visualize and choose appropriate chart types
4. **Data Transformation**: Filter, sort, and manipulate previous results without re-querying
5. **Conversation Persistence**: Save and restore complete conversation sessions
6. **Metadata-Driven Intelligence**: Use LLM-analyzed metadata for better SQL generation

### Technology Stack
- **Language**: Python 3.10+
- **LLM Framework**: LangChain + Ollama (qwen2.5:7b model)
- **Vector Database**: ChromaDB (with nomic-embed-text embeddings, 768-dim)
- **SQL Database**: SQLite
- **Visualization**: Plotly (interactive charts)
- **Web Interface**: Gradio
- **Data Processing**: Pandas

---

## HIGH-LEVEL ARCHITECTURE

### System Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          PRESENTATION LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Gradio Web Interface (app_gradio_enhanced.py)                   â”‚   â”‚
â”‚  â”‚  - Chat Interface with History Sidebar                           â”‚   â”‚
â”‚  â”‚  - Interactive Visualizations (Plotly Charts)                    â”‚   â”‚
â”‚  â”‚  - Data Tables (HTML)                                            â”‚   â”‚
â”‚  â”‚  - Conversation Management (New/Load/Delete)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          APPLICATION LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  QueryAgentEnhanced (QueryAgent_Ollama_Enhanced.py)             â”‚   â”‚
â”‚  â”‚  - Question Intent Analysis                                      â”‚   â”‚
â”‚  â”‚  - SQL Generation & Validation                                   â”‚   â”‚
â”‚  â”‚  - Result Processing                                             â”‚   â”‚
â”‚  â”‚  - Visualization Decision Making                                 â”‚   â”‚
â”‚  â”‚  - Context Management                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ConversationState (conversation_manager.py)                     â”‚   â”‚
â”‚  â”‚  - Message History Tracking                                      â”‚   â”‚
â”‚  â”‚  - Data Context Storage                                          â”‚   â”‚
â”‚  â”‚  - Visualization Records                                         â”‚   â”‚
â”‚  â”‚  - Import/Export Functionality                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          INTELLIGENCE LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Ollama LLM (qwen2.5:7b)                                         â”‚   â”‚
â”‚  â”‚  - Intent Classification                                         â”‚   â”‚
â”‚  â”‚  - SQL Query Generation                                          â”‚   â”‚
â”‚  â”‚  - Natural Language Answer Generation                            â”‚   â”‚
â”‚  â”‚  - Visualization Recommendations                                 â”‚   â”‚
â”‚  â”‚  - Metadata Analysis (during setup)                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LangChain Framework                                             â”‚   â”‚
â”‚  â”‚  - Prompt Templates                                              â”‚   â”‚
â”‚  â”‚  - Output Parsers (Pydantic Models)                              â”‚   â”‚
â”‚  â”‚  - SQL Query Chain                                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA LAYER                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ChromaDB       â”‚  â”‚  SQLite         â”‚  â”‚  Conversation Files    â”‚  â”‚
â”‚  â”‚  (Vector DB)    â”‚  â”‚  (Source DB)    â”‚  â”‚  (JSON)                â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                        â”‚  â”‚
â”‚  â”‚  â€¢ Table        â”‚  â”‚  â€¢ User Tables  â”‚  â”‚  â€¢ Message History     â”‚  â”‚
â”‚  â”‚    Metadata     â”‚  â”‚  â€¢ Business     â”‚  â”‚  â€¢ Data Snapshots      â”‚  â”‚
â”‚  â”‚  â€¢ Column       â”‚  â”‚    Data         â”‚  â”‚  â€¢ Visualizations      â”‚  â”‚
â”‚  â”‚    Metadata     â”‚  â”‚                 â”‚  â”‚  â€¢ Timestamps          â”‚  â”‚
â”‚  â”‚  â€¢ Embeddings   â”‚  â”‚                 â”‚  â”‚                        â”‚  â”‚
â”‚  â”‚    (768-dim)    â”‚  â”‚                 â”‚  â”‚                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interaction Flow

```
User Question
    â†“
[Gradio UI] â†’ process_question()
    â†“
[QueryAgentEnhanced] â†’ answer_question_with_context()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Intent Analysis                       â”‚
â”‚ _analyze_question_intent()                    â”‚
â”‚ â€¢ Detect: NEW_QUERY, RE_VISUALIZE,            â”‚
â”‚   TRANSFORM, COMBINE, COMPARE, CLARIFY        â”‚
â”‚ â€¢ Check for previous data references          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Context Retrieval                     â”‚
â”‚ â€¢ Search Vector DB for relevant metadata      â”‚
â”‚ â€¢ Build context from conversation history     â”‚
â”‚ â€¢ Retrieve previous DataFrame if referenced   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Query Processing (Intent-Based)       â”‚
â”‚                                                â”‚
â”‚ NEW_QUERY â†’ _process_new_query()              â”‚
â”‚   â”œâ†’ Generate SQL via LLM                     â”‚
â”‚   â”œâ†’ Clean & Validate SQL                     â”‚
â”‚   â”œâ†’ Execute Query                            â”‚
â”‚   â””â†’ Return DataFrame                         â”‚
â”‚                                                â”‚
â”‚ RE_VISUALIZE â†’ _handle_revisualization()      â”‚
â”‚   â”œâ†’ Apply previous filters                   â”‚
â”‚   â”œâ†’ Generate new visualization               â”‚
â”‚   â””â†’ Return with preserved context            â”‚
â”‚                                                â”‚
â”‚ TRANSFORM â†’ _handle_transformation()          â”‚
â”‚   â”œâ†’ Generate pandas transformation code      â”‚
â”‚   â”œâ†’ Execute safely                           â”‚
â”‚   â””â†’ Return transformed DataFrame             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Answer Generation                     â”‚
â”‚ _generate_answer()                            â”‚
â”‚ â€¢ LLM generates natural language response     â”‚
â”‚ â€¢ Includes data insights and statistics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Visualization Decision                â”‚
â”‚ _should_visualize()                           â”‚
â”‚ â€¢ Analyze question for viz keywords           â”‚
â”‚ â€¢ LLM recommends chart type                   â”‚
â”‚ â€¢ Return VisualizationResponse model          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Chart Creation                        â”‚
â”‚ _create_visualization()                       â”‚
â”‚ â€¢ Create Plotly figure based on type          â”‚
â”‚ â€¢ Configure axes, colors, legends             â”‚
â”‚ â€¢ Return interactive chart                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: State Update                          â”‚
â”‚ _update_conversation_state()                  â”‚
â”‚ â€¢ Add user message                            â”‚
â”‚ â€¢ Add assistant message with metadata         â”‚
â”‚ â€¢ Store DataFrame snapshot                    â”‚
â”‚ â€¢ Store visualization as JSON                 â”‚
â”‚ â€¢ Update data contexts                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: Conversation Persistence              â”‚
â”‚ save_current_conversation()                   â”‚
â”‚ â€¢ Export conversation state to JSON           â”‚
â”‚ â€¢ Save to conversations/<uuid>.json           â”‚
â”‚ â€¢ Update conversation list in UI              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Gradio UI] Display:
â”œâ†’ Text Answer
â”œâ†’ Interactive Chart (Plotly)
â””â†’ Data Table (HTML)

â””â†’ Data Table (HTML)
```

---

## CORE COMPONENTS DEEP DIVE

### 1. QueryAgentEnhanced Class

**File**: `QueryAgent_Ollama_Enhanced.py`  
**Primary Responsibility**: Orchestrate question answering with context awareness

#### Constructor Parameters

```python
QueryAgentEnhanced(
    source_db_path: str,              # Path to SQLite database
    vector_db_path: str = "./chroma_db_768dim",  # Path to ChromaDB
    llm_model: str = "qwen2.5:7b",    # Ollama model name
    conversation_state: Optional[ConversationState] = None,
    max_context_messages: int = 10,   # Max messages to keep in context
    max_data_contexts: int = 20,      # Max data contexts to retain
    temperature: float = 0.1,         # LLM temperature (0.0-1.0)
    ollama_base_url: str = "http://localhost:11434",
    embedding_model: str = "nomic-embed-text"  # 768-dim embeddings
)
```

**What Happens During Initialization:**

1. **Database Connection**:
   ```python
   self.conn = sqlite3.connect(source_db_path, check_same_thread=False)
   self.db = SQLDatabase.from_uri(f"sqlite:///{source_db_path}")
   ```
   - Creates persistent SQLite connection
   - Initializes LangChain SQLDatabase wrapper
   - Allows thread-safe operations

2. **Vector Database Setup**:
   ```python
   self._setup_vector_database()
   ```
   - Connects to ChromaDB at specified path
   - Retrieves `table_metadata` and `column_metadata` collections
   - Loads all metadata into memory for fast access

3. **LLM Initialization**:
   ```python
   self.llm = ChatOllama(
       model=llm_model,
       base_url=ollama_base_url,
       temperature=temperature,
       num_ctx=4096,          # Context window size
       num_predict=1024,      # Max tokens to generate
       repeat_penalty=1.1,    # Prevent repetition
       timeout=120            # Request timeout
   )
   ```

4. **Chain Creation**:
   ```python
   self.query_chain = create_sql_query_chain(self.llm, self.db)
   ```
   - Creates LangChain SQL query generation chain
   - Automatically includes schema information

5. **Parser Setup**:
   ```python
   self.viz_parser = PydanticOutputParser(pydantic_object=VisualizationResponse)
   ```
   - Initializes structured output parsers
   - Ensures LLM responses match expected format

#### Core Methods

##### `answer_question_with_context(question: str, reuse_data: bool = False) -> Dict[str, Any]`

**Purpose**: Main entry point for processing user questions with full context awareness.

**Algorithm Flow**:

```
INPUT: User question string
â†“
1. Analyze Intent
   _analyze_question_intent(question)
   â†“
   â€¢ Send question + recent conversation to LLM
   â€¢ LLM returns IntentAnalysis:
     - intent: NEW_QUERY | RE_VISUALIZE | TRANSFORM | COMBINE | COMPARE | CLARIFY
     - references_previous: bool
     - referenced_concepts: List[str]
     - needs_context: bool
     - confidence: float (0.0-1.0)
   
2. Check Previous Reference
   _check_previous_reference(question)
   â†“
   â€¢ Search for keywords: "that", "this", "previous", etc.
   â€¢ If found, retrieve latest DataFrame from conversation_state
   â€¢ Return (references_found: bool, df: Optional[DataFrame])

3. Route Based on Intent & Data Availability
   â†“
   â”œâ”€ [Has Previous DF + RE_VISUALIZE Intent]
   â”‚  â””â†’ _handle_revisualization(question, df)
   â”‚     â”œâ†’ Extract filters from previous SQL
   â”‚     â”œâ†’ Apply filters to DataFrame
   â”‚     â”œâ†’ Generate new visualization
   â”‚     â””â†’ Preserve context and filters
   â”‚
   â”œâ”€ [Has Previous DF + TRANSFORM Intent]
   â”‚  â””â†’ _handle_transformation(question, df)
   â”‚     â”œâ†’ LLM generates pandas code
   â”‚     â”œâ†’ Execute in safe environment
   â”‚     â””â†’ Return transformed data
   â”‚
   â””â”€ [NEW_QUERY or No Previous DF]
      â””â†’ _process_new_query(question, intent)
         â”œâ†’ Search vector DB for metadata
         â”œâ†’ Build context prompt
         â”œâ†’ Generate SQL with retry logic
         â”œâ†’ Clean & validate SQL
         â”œâ†’ Execute query
         â””â†’ Return results

4. Generate Natural Language Answer
   _generate_answer(question, df, sql_query)
   â†“
   â€¢ Create prompt with question + data summary
   â€¢ LLM generates insights and explanations
   â€¢ Return human-readable answer

5. Step5: Determine Visualization
   _should_visualize(question, df)
   â†“
   â€¢ Check for viz keywords in question
   â€¢ If requested, LLM analyzes data structure
   â€¢ Returns VisualizationResponse with:
     - should_visualize: bool
     - primary_chart: str (bar/line/pie/scatter/box/etc.)
     - x_axis, y_axis, color_by: column names
     - title: str
     - rationale: str

6. Create Visualization
   _create_visualization(df, viz_response)
   â†“
   â€¢ Based on chart type, create Plotly figure
   â€¢ Configure styling, axes, legends
   â€¢ Return interactive chart object

7. Update Conversation State
   _update_conversation_state(question, result, intent)
   â†“
   â€¢ Add Message objects for user and assistant
   â€¢ Create DataFrame snapshot (first 50 rows as dict)
   â€¢ Serialize Plotly figure to JSON string
   â€¢ Add DataContext with query info
   â€¢ Add VisualizationRecord
   â€¢ Auto-cleanup old contexts (keep last 10)

OUTPUT: Dict {
    success: bool,
    question: str,
    sql_query: str,
    answer: str,
    data: DataFrame,
    visualization: {chart: Figure, type: str, rationale: str},
    intent: str,
    reused_data: bool,
    conversation_id: str
}
```

**Key Features**:
- **Retry Logic**: SQL generation retries up to 2 times on failure
- **Error Recovery**: If execution fails, feeds error back to LLM for correction
- **Context Preservation**: Maintains conversation flow across multiple turns
- **Smart Caching**: Reuses previous DataFrames when appropriate

##### `_analyze_question_intent(question: str) -> IntentAnalysis`

**Purpose**: Classify user's question to determine processing strategy.

**Implementation**:

```python
def _analyze_question_intent(self, question: str) -> IntentAnalysis:
    # Get recent conversation context
    recent_messages = self.conversation_state.get_recent_messages(10)
    context_str = "\n".join([f"- {msg.role}: {msg.content}" for msg in recent_messages])
    
    # Build intent analysis prompt
    intent_prompt = f"""Analyze this question and determine the user's intent.

Recent conversation:
{context_str if context_str else "No previous conversation"}

Current question: "{question}"

Keywords indicating intent:
- NEW_QUERY: "show me", "what are", "list all", "find", "get"
- RE_VISUALIZE: "show as", "visualize as", "make a", "create chart", "different chart"
- TRANSFORM: "calculate", "add", "filter", "sort", "group by"
- COMBINE: "merge with", "combine", "join with", "add to"
- COMPARE: "compare", "difference between", "vs", "versus"
- CLARIFY: "what do you mean", "explain", "why"

Words indicating reference to previous: "that", "this", "these", "those", "previous", "last", "earlier", "above"

Return JSON with:
- intent: one of [new_query, re_visualize, transform, combine, compare, clarify]
- references_previous: true/false
- referenced_concepts: list of concepts mentioned (e.g., ["sales", "products"])
- needs_context: true if previous data is needed
- confidence: 0.0 to 1.0

Only return valid JSON, no other text."""
    
    # Invoke LLM
    response = self.llm.invoke(intent_prompt)
    content = response.content.strip()
    
    # Extract JSON from response (handles markdown code blocks)
    if "```json" in content:
        content = content.split("```json")[1].split("```")[0].strip()
    elif "```" in content:
        content = content.split("```")[1].split("```")[0].strip()
    
    # Parse to Pydantic model
    intent_data = json.loads(content)
    return IntentAnalysis(**intent_data)
```

**Output Example**:
```python
IntentAnalysis(
    intent="re_visualize",
    references_previous=True,
    referenced_concepts=["profit margin", "categories"],
    needs_context=True,
    confidence=0.9
)
```

##### `_generate_sql_with_retry(question: str, context_prompt: str, metadata_str: str, max_retries: int = 2) -> str`

**Purpose**: Generate SQL with automatic retry on failure.

**Algorithm**:

```
FOR attempt IN range(max_retries):
    TRY:
        1. Combine context_prompt + metadata_str
        2. Invoke query_chain with combined prompt
        3. IF sql_query is empty:
           - Log warning
           - Continue to next attempt
        4. RETURN sql_query
    
    EXCEPT Exception as e:
        - Log error with attempt number
        - IF last attempt:
          - RAISE exception
        - ELSE:
          - Continue to next attempt

IF all attempts fail:
    RAISE Exception("Failed to generate SQL after all retries")
```

##### `_clean_sql(query: str) -> str`

**Purpose**: Comprehensive SQL cleaning and sanitization.

**Steps**:

1. **Remove Markdown Fences**:
   ```python
   q = re.sub(r"```\w*", "", q)
   ```

2. **Extract Last SQL Marker**:
   ```python
   marker_iter = list(re.finditer(r"(?i)(sqlquery|sql query|sql|query)\s*:", q))
   if marker_iter:
       q = q[marker_iter[-1].end():]
   ```

3. **Remove Explanatory Text**:
   ```python
   q = re.sub(r"(?i)^.*?(?:here'?s?\s+(?:the\s+)?(?:sql\s+)?query[:\s]+)", "", q)
   ```

4. **Extract SELECT/WITH**:
   ```python
   cte_match = re.search(r"(?i)\bwith\s+[A-Za-z_][\w]*\s+as", q)
   select_match = re.search(r"(?i)\bselect\b", q)
   
   if cte_match and (not select_match or cte_match.start() <= select_match.start()):
       q = q[cte_match.start():]
   elif select_match:
       q = q[select_match.start():]
   ```

5. **Balance Quotes**:
   ```python
   single_quote_count = query.count("'")
   if single_quote_count % 2 == 1:
       query = query + "'"
   ```

6. **Fix Common Syntax Issues**:
   - Adjacent quoted strings: `AS "Total" "Sales"` â†’ `AS "Total Sales"`
   - Unquoted aliases with spaces
   - Missing closing quotes in GROUP BY/ORDER BY

7. **Collapse Whitespace**:
   ```python
   q = re.sub(r"\s+", " ", q).strip()
   ```

##### `_validate_and_fix_tables(sql_query: str) -> str`

**Purpose**: Validate table/column references and fix invalid ones.

**Algorithm**:

```
1. Get Valid Tables
   - Query SQLite master table
   - Extract CTE names from query
   - Combine into valid_tables set

2. Build Column Map
   FOR each table IN valid_tables:
       - Get columns via PRAGMA table_info
       - Map column_name.lower() â†’ (table, original_column_name)

3. Analyze Query Aliases
   _analyze_query_aliases(sql_query)
   - Extract FROM/JOIN aliases
   - Track: valid_aliases, base_to_alias, alias_to_table

4. Find Invalid JOIN Tables
   - Scan for JOIN patterns
   - Identify tables not in valid_tables
   - Mark as invalid_tables

5. Fix Invalid References
   IF invalid_tables exist:
       - Remove JOIN clauses with invalid tables
       - Replace invalid_table.column with valid_table.column
       - Fix aggregate functions: SUM(invalid.col) â†’ SUM(valid.col)
       - Cleanup broken clauses (empty WHERE, trailing commas)

6. Fix Window Functions
   - Detect SUM(alias.column) OVER ()
   - Verify alias and column exist in subquery outputs
   - Replace with correct column references

7. Replace Unknown Aliases
   - Find all table.column references
   - If table not in valid_aliases:
     - Look up in base_to_alias mapping
     - Replace with known alias

RETURN: Fixed SQL query
```

**Example Fix**:
```sql
-- BEFORE (Invalid)
SELECT 
    s.region, 
    SUM(invalid_table.amount) as total
FROM sales s
LEFT JOIN invalid_table ON s.id = invalid_table.sale_id
GROUP BY s.region

-- AFTER (Fixed)
SELECT 
    s.region, 
    SUM(s.amount) as total
FROM sales s
GROUP BY s.region
```

##### `_should_visualize(question: str, df: pd.DataFrame) -> VisualizationResponse`

**Purpose**: Determine if visualization is appropriate and recommend type.

**Decision Logic**:

```
1. Early Rejection Checks:
   IF df.empty OR len(df) < 2:
       RETURN VisualizationResponse(
           should_visualize=False,
           rationale="Not enough data"
       )

2. Keyword Detection:
   viz_keywords = ['visualize', 'plot', 'chart', 'graph', 'show', 
                   'display', 'pie', 'bar', 'line', 'scatter']
   IF NO keywords in question:
       RETURN should_visualize=False

3. LLM Analysis:
   Prompt:
   - Question
   - Data info (rows, columns, types)
   - Sample data (first 5 rows)
   
   LLM Returns:
   {
       "should_visualize": true,
       "chart_types": ["bar", "line"],
       "primary_chart": "bar",
       "x_axis": "category",
       "y_axis": "sales",
       "color_by": "region",
       "title": "Sales by Category",
       "visualization_rationale": "Bar chart best shows comparison across categories"
   }

4. Fallback to Simple Rules:
   IF LLM fails:
       - Box plot if "box" or "distribution" in question
       - Bar chart as default
       - Auto-select first categorical for X, first numeric for Y
```

##### `_create_visualization(df: pd.DataFrame, viz_response: VisualizationResponse) -> go.Figure`

**Purpose**: Create Plotly visualization based on recommendations.

**Chart Types Supported**:

1. **Bar Chart**:
   ```python
   fig = px.bar(df, x=x_axis, y=y_axis, color=color_by, title=title)
   if color_by:
       fig.update_layout(showlegend=True)
   ```

2. **Multiple Bar Chart** (Grouped):
   ```python
   # If y_axis is a list of columns
   df_melted = df.melt(
       id_vars=[x_axis],
       value_vars=y_axis,
       var_name='Series',
       value_name='Value'
   )
   fig = px.bar(df_melted, x=x_axis, y='Value', color='Series', 
                barmode='group', title=title)
   ```

3. **Line Chart**:
   ```python
   fig = px.line(df, x=x_axis, y=y_axis, color=color_by, 
                 title=title, markers=True)
   ```

4. **Pie Chart**:
   ```python
   # Limit to top 10 for readability
   df_pie = df.nlargest(10, y_axis) if len(df) > 10 else df
   fig = px.pie(df_pie, names=x_axis, values=y_axis, title=title)
   fig.update_traces(textposition='inside', textinfo='percent+label')
   fig.update_layout(showlegend=True)
   ```

5. **Scatter Plot**:
   ```python
   fig = px.scatter(df, x=x_axis, y=y_axis, color=color_by, title=title)
   ```

6. **Histogram**:
   ```python
   fig = px.histogram(df, x=x_axis, title=title)
   ```

7. **Box Plot**:
   ```python
   fig = px.box(df, x=x_axis, y=y_axis, color=x_axis, 
                title=title, points="outliers")
   fig.update_traces(marker=dict(size=4, opacity=0.6), boxmean='sd')
   ```

**Common Styling**:
```python
fig.update_layout(
    template="plotly",
    title_font_size=16,
    showlegend=True,
    hovermode='closest',
    height=500,
    xaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray'),
    yaxis=dict(showgrid=True, gridwidth=1, gridcolor='LightGray'),
    legend=dict(
        orientation="v",
        yanchor="top",
        y=0.99,
        xanchor="left",
        x=1.01,
        bgcolor="rgba(255, 255, 255, 0.8)",
        bordercolor="rgba(0, 0, 0, 0.2)",
        borderwidth=1
    )
)
```

---

### 2. ConversationState Class

**File**: `conversation_manager.py`  
**Primary Responsibility**: Manage conversation history, data contexts, and persistence

#### Data Classes

##### `Message`
```python
@dataclass
class Message:
    role: str              # "user" or "assistant"
    content: str           # Message text
    timestamp: datetime    # When message was created
    sql_query: Optional[str] = None           # Generated SQL (assistant only)
    dataframe_snapshot: Optional[Dict] = None # Data metadata
    visualization: Optional[str] = None       # Chart type
    figure_json: Optional[str] = None         # Plotly figure as JSON
    metadata: Dict = field(default_factory=dict)
```

**DataFrame Snapshot Structure**:
```python
{
    "columns": ["col1", "col2", "col3"],
    "row_count": 1000,
    "sample": {
        "col1": {"0": "val1", "1": "val2", ...},  # First 50 rows
        "col2": {"0": 10, "1": 20, ...},
        "col3": {"0": "A", "1": "B", ...}
    }
}
```

##### `DataContext`
```python
@dataclass
class DataContext:
    query: str             # SQL query executed
    columns: List[str]     # Column names in result
    row_count: int         # Number of rows returned
    sample_data: Dict      # First 5 rows as dict
    timestamp: datetime    # When context was created
```

##### `VisualizationRecord`
```python
@dataclass
class VisualizationRecord:
    question: str          # User's question
    chart_type: str        # Type of chart created
    data_summary: str      # Brief description of data
    timestamp: datetime    # When viz was created
```

#### Core Methods

##### `add_message(message: Message)`

**Purpose**: Add a message to conversation history.

**Implementation**:
```python
def add_message(self, message: Message):
    self.messages.append(message)
```

**Usage Pattern**:
```python
# User message
conversation_state.add_message(Message(
    role="user",
    content="What are the top 5 sales?",
    metadata={"intent": "new_query"}
))

# Assistant message
conversation_state.add_message(Message(
    role="assistant",
    content="Here are the top 5 sales...",
    sql_query="SELECT * FROM sales ORDER BY amount DESC LIMIT 5",
    dataframe_snapshot={
        "columns": ["id", "amount", "date"],
        "row_count": 5,
        "sample": {...}
    },
    visualization="bar",
    figure_json='{"data": [...], "layout": {...}}',
    metadata={"intent": "new_query", "success": True}
))
```

##### `add_data_context(context: DataContext)`

**Purpose**: Store query result metadata.

**Implementation**:
```python
def add_data_context(self, context: DataContext):
    self.data_contexts.append(context)
    # Keep only last 10 contexts to manage memory
    if len(self.data_contexts) > 10:
        self.data_contexts = self.data_contexts[-10:]
```

**Auto-Cleanup**: Automatically limits to last 10 contexts to prevent memory bloat.

##### `get_recent_messages(n: int = 5) -> List[Message]`

**Purpose**: Retrieve last N messages for context building.

**Implementation**:
```python
def get_recent_messages(self, n: int = 5) -> List[Message]:
    return self.messages[-n:] if len(self.messages) >= n else self.messages
```

**Usage**: Used by `_analyze_question_intent()` and `_build_context_prompt()` to provide conversation history to LLM.

##### `export_conversation() -> Dict`

**Purpose**: Serialize conversation state for persistence.

**Implementation**:
```python
def export_conversation(self) -> Dict:
    return {
        "conversation_id": self.conversation_id,
        "start_time": self.start_time.isoformat(),
        "message_count": len(self.messages),
        "messages": [
            {
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp.isoformat(),
                "sql_query": msg.sql_query,
                "dataframe_snapshot": msg.dataframe_snapshot,
                "visualization": msg.visualization,
                "figure_json": msg.figure_json,
                "metadata": msg.metadata
            }
            for msg in self.messages
        ],
        "data_contexts": [
            {
                "query": ctx.query,
                "columns": ctx.columns,
                "row_count": ctx.row_count,
                "sample_data": ctx.sample_data,
                "timestamp": ctx.timestamp.isoformat()
            }
            for ctx in self.data_contexts
        ],
        "visualizations": [
            {
                "question": viz.question,
                "chart_type": viz.chart_type,
                "data_summary": viz.data_summary,
                "timestamp": viz.timestamp.isoformat()
            }
            for viz in self.visualizations
        ]
    }
```

**Output Format**: See [Data Structures & Examples](#data-structures--examples) section for full JSON example.

##### `import_conversation(data: Dict)`

**Purpose**: Restore conversation state from JSON.

**Algorithm**:
```
1. Parse conversation_id and start_time
2. FOR each message in data["messages"]:
   - Create Message object
   - Parse ISO timestamps to datetime
   - Restore all fields including metadata
3. FOR each context in data["data_contexts"]:
   - Create DataContext object
   - Parse timestamps
4. FOR each viz in data["visualizations"]:
   - Create VisualizationRecord object
5. Update self.messages, self.data_contexts, self.visualizations
```

---

### 3. Gradio UI (app_gradio_enhanced.py)

**Primary Responsibility**: Web interface for user interaction

#### Global State Management

```python
# Global variables (module-level)
agent: Optional[QueryAgentEnhanced] = None
conversation_state: Optional[ConversationState] = None
CONVERSATIONS_DIR = "conversations"
```

**Why Global**: Gradio functions are stateless; global state persists across invocations.

#### Key Functions

##### `initialize_agent(db_path: str, vector_db: str, model: str) -> Tuple[str, gr.update]`

**Purpose**: Initialize the QueryAgentEnhanced with user-specified parameters.

**Steps**:
1. Validate database file exists
2. Create new ConversationState
3. Initialize QueryAgentEnhanced with parameters
4. Load conversation list from disk
5. Return status message and updated history dropdown

**Returns**:
- Status message: "âœ… Agent Ready" or error
- Gradio update for history dropdown

##### `process_question(question: str, history: List) -> Tuple[List, gr.update]`

**Purpose**: Process user question and update chat history.

**Algorithm**:
```
INPUT: question (str), history (List of tuples)

1. Validate agent exists
2. Call agent.answer_question_with_context(question)
3. Extract result components:
   - answer (text)
   - visualization (Plotly figure)
   - data (DataFrame)

4. Update history list:
   history.append((question, answer))
   
   IF visualization exists:
       history.append((None, gr.Plot(value=fig)))
   
   IF data exists:
       df_display = data.head(100)
       html_table = df_display.to_html()
       history.append((None, table_html))

5. Save conversation to disk:
   save_current_conversation()

6. Update conversation list in sidebar:
   new_list = get_conversation_list()

OUTPUT: (updated_history, gr.update(choices=new_list))
```

**History Format**:
```python
[
    ("User question 1", "Assistant answer 1"),
    (None, gr.Plot(...)),                    # Chart
    (None, "<div>HTML Table</div>"),        # Data table
    ("User question 2", "Assistant answer 2"),
    ...
]
```

##### `load_conversation(filename: str) -> Tuple[List, str]`

**Purpose**: Load and reconstruct a saved conversation.

**Algorithm**:
```
1. Read JSON file from conversations/<filename>
2. Create new ConversationState
3. Import data using conversation_state.import_conversation(data)
4. Update global conversation_state and agent.conversation_state
5. Reconstruct history for display:
   FOR each message in conversation_state.messages:
       IF role == "user":
           pending_user = content
       ELIF role == "assistant":
           - Add (user, assistant) text tuple
           - IF figure_json exists:
               - Parse JSON to Plotly figure
               - Add (None, gr.Plot(fig)) tuple
           - IF dataframe_snapshot exists:
               - Reconstruct DataFrame from sample
               - Convert to HTML table
               - Add (None, table_html) tuple
6. RETURN (history, status_message)
```

**Reconstruction Challenges**:
- Plotly figures stored as JSON strings must be deserialized
- DataFrames reconstructed from sample data (only first 50 rows saved)
- Order must be preserved: user â†’ assistant â†’ chart â†’ table

##### `save_current_conversation()`

**Purpose**: Persist current conversation to disk.

**Implementation**:
```python
def save_current_conversation():
    if conversation_state and conversation_state.messages:
        ensure_conversations_dir()  # Create dir if not exists
        data = conversation_state.export_conversation()
        filename = f"{conversation_state.conversation_id}.json"
        filepath = os.path.join(CONVERSATIONS_DIR, filename)
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
```

**Trigger Points**:
- After every successful question processing
- Automatic (no user action required)

##### `get_conversation_list() -> List[Tuple[str, str]]`

**Purpose**: Get list of saved conversations for sidebar dropdown.

**Implementation**:
```python
def get_conversation_list() -> List[Tuple[str, str]]:
    ensure_conversations_dir()
    files = [f for f in os.listdir(CONVERSATIONS_DIR) if f.endswith('.json')]
    files.sort(key=lambda x: os.path.getmtime(os.path.join(CONVERSATIONS_DIR, x)), 
               reverse=True)
    
    choices = []
    for f in files:
        path = os.path.join(CONVERSATIONS_DIR, f)
        with open(path, 'r') as file:
            data = json.load(file)
        
        # Extract title from first user message
        title = "New Conversation"
        for msg in data.get('messages', []):
            if msg['role'] == 'user':
                title = msg['content'][:30] + "..." if len(msg['content']) > 30 else msg['content']
                break
        
        # Format timestamp
        ts = data.get('start_time', datetime.now().isoformat())
        dt = datetime.fromisoformat(ts)
        date_str = dt.strftime("%m/%d %H:%M")
        
        label = f"{date_str} - {title}"
        choices.append((label, f))  # (display_label, filename)
    
    return choices
```

**Output Example**:
```python
[
    ("11/20 22:50 - I'm trying to understand ou...", "2e6a2e96-1a3f-43f8-95cb-2a1debc1135a.json"),
    ("11/19 15:30 - Show me top 5 sales...", "df8d33d4-744a-4f66-8330-d03ce7f17844.json"),
    ...
]
```

#### UI Layout

**Sidebar** (Left Column):
```python
with gr.Column(scale=1, elem_classes="sidebar", min_width=300):
    gr.Markdown("### ğŸ—„ï¸ Chat History")
    with gr.Row():
        new_chat_btn = gr.Button("+ New Chat")
        delete_btn = gr.Button("ğŸ—‘ï¸")
    
    history_list = gr.Radio(
        label="Recent Conversations",
        choices=get_conversation_list(),
        interactive=True
    )
    
    with gr.Accordion("âš™ï¸ Settings", open=True):
        db_input = gr.Textbox(label="Database Path", value="analysis.db")
        vec_input = gr.Textbox(label="Vector DB", value="./chroma_db_768dim")
        model_input = gr.Dropdown(label="Model", choices=["qwen2.5:7b", "llama3", "mistral"])
        init_btn = gr.Button("Initialize the LLM Agent")
        init_status = gr.Markdown("Not Connected")
```

**Main Chat Area** (Right Column):
```python
with gr.Column(scale=4, elem_classes="chat-area"):
    chatbot = gr.Chatbot(
        height=750,
        avatar_images=(None, "https://cdn-icons-png.flaticon.com/512/4712/4712027.png"),
        render_markdown=True,
        type="tuples",
        sanitize_html=False  # Allow HTML tables
    )
    
    with gr.Row():
        msg_input = gr.Textbox(
            scale=9,
            placeholder="Ask a question about your data...",
            lines=1
        )
        submit_btn = gr.Button("â¤", scale=1, variant="primary")
```

#### Event Wiring

```python
# Initialize agent
init_btn.click(
    fn=initialize_agent,
    inputs=[db_input, vec_input, model_input],
    outputs=[init_status, history_list]
)

# New chat
new_chat_btn.click(
    fn=start_new_chat,
    outputs=[chatbot, init_status]
)

# Delete current conversation
delete_btn.click(
    fn=delete_conversation,
    outputs=[chatbot, history_list, init_status]
)

# Load conversation from sidebar
history_list.select(
    fn=load_conversation,
    inputs=[history_list],
    outputs=[chatbot, init_status]
)

# Submit question (two triggers: button click and Enter key)
submit_args = {
    "fn": process_question,
    "inputs": [msg_input, chatbot],
    "outputs": [chatbot, history_list]
}
msg_input.submit(**submit_args).then(lambda: "", outputs=msg_input)  # Clear input
submit_btn.click(**submit_args).then(lambda: "", outputs=msg_input)
```

#### Custom CSS

```css
.sidebar {
    padding: 20px;
    border-right: 1px solid var(--border-color-primary);
    height: 100vh;
    overflow-y: auto;
}

.chat-area {
    padding: 20px;
    height: 100vh;
    overflow-y: auto;
}

.data-table {
    width: 100%;
    border-collapse: collapse;
    font-size: 0.9em;
}

.data-table th {
    background-color: var(--background-fill-secondary);
    padding: 8px;
    text-align: left;
    border-bottom: 2px solid var(--border-color-primary);
}

.data-table td {
    padding: 8px;
    border-bottom: 1px solid var(--border-color-primary);
}

.table-container {
    overflow-x: auto;
    border: 1px solid var(--border-color-primary);
    border-radius: 8px;
    padding: 10px;
}
```

---

## DATA FLOW & WORKFLOWS

### Workflow 1: First-Time Setup (Database Analysis)

**Script**: `analyze_existing_db.py`  
**Purpose**: One-time analysis to create vector database with metadata

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: User Runs Analyzer                            â”‚
â”‚ $ python analyze_existing_db.py analysis.db           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Initialize Connections                         â”‚
â”‚ â€¢ Connect to source SQLite database                    â”‚
â”‚ â€¢ Initialize Ollama LLM (qwen2.5:7b)                   â”‚
â”‚ â€¢ Create/connect to ChromaDB                           â”‚
â”‚ â€¢ Setup prompt templates                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Discover Tables                                â”‚
â”‚ get_table_names()                                      â”‚
â”‚ â€¢ Query sqlite_master for table names                  â”‚
â”‚ â€¢ Exclude system tables (sqlite_%)                     â”‚
â”‚ â€¢ Return list: ["sales", "products", "customers"]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: For Each Table - Analyze Metadata             â”‚
â”‚ _analyze_table_metadata(table_name, df)               â”‚
â”‚                                                        â”‚
â”‚ LLM Input:                                             â”‚
â”‚ - Table name                                           â”‚
â”‚ - Column list                                          â”‚
â”‚ - First 5 rows sample                                  â”‚
â”‚ - Row count                                            â”‚
â”‚                                                        â”‚
â”‚ LLM Output (MetadataResponse):                         â”‚
â”‚ {                                                      â”‚
â”‚   "table_name": "sales",                               â”‚
â”‚   "description": "Transaction records for product      â”‚
â”‚                   sales with timestamps and amounts",  â”‚
â”‚   "category": "financial",                             â”‚
â”‚   "business_context": "Core revenue tracking system",  â”‚
â”‚   "suggested_primary_key": "transaction_id",           â”‚
â”‚   "data_quality_notes": [                              â”‚
â”‚     "Some null values in customer_id",                 â”‚
â”‚     "Date range: 2023-01-01 to 2024-12-31"            â”‚
â”‚   ]                                                    â”‚
â”‚ }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: For Each Column - Analyze Details             â”‚
â”‚ _analyze_column(table_name, column_name, column_data) â”‚
â”‚                                                        â”‚
â”‚ LLM Input:                                             â”‚
â”‚ - Table and column names                               â”‚
â”‚ - Sample values (first 10)                             â”‚
â”‚ - Unique count, null count                             â”‚
â”‚                                                        â”‚
â”‚ LLM Output (DataTypeResponse):                         â”‚
â”‚ {                                                      â”‚
â”‚   "sql_type": "REAL",                                  â”‚
â”‚   "python_type": "float",                              â”‚
â”‚   "description": "Transaction amount in USD",          â”‚
â”‚   "business_meaning": "Revenue generated from sale",   â”‚
â”‚   "constraints": ["NOT NULL", "CHECK > 0"],            â”‚
â”‚   "is_nullable": false,                                â”‚
â”‚   "suggested_index": true                              â”‚
â”‚ }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Generate Embeddings                            â”‚
â”‚ _get_embedding(text)                                   â”‚
â”‚                                                        â”‚
â”‚ For Table:                                             â”‚
â”‚ text = "Table: sales\nDescription: Transaction         â”‚
â”‚         records...\nCategory: financial..."            â”‚
â”‚ â†’ POST to Ollama /api/embeddings                       â”‚
â”‚ â†’ Model: nomic-embed-text                              â”‚
â”‚ â†’ Returns: [0.123, -0.456, ...] (768 dimensions)       â”‚
â”‚                                                        â”‚
â”‚ For Each Column:                                       â”‚
â”‚ text = "Table: sales\nColumn: amount\nType: REAL..."   â”‚
â”‚ â†’ Generate 768-dim embedding                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: Store in ChromaDB                              â”‚
â”‚ save_analysis(analysis)                                â”‚
â”‚                                                        â”‚
â”‚ Table Collection:                                      â”‚
â”‚ â€¢ ID: "table_sales"                                    â”‚
â”‚ â€¢ Document: "Table: sales\nDescription: ..."           â”‚
â”‚ â€¢ Embedding: [768 floats]                              â”‚
â”‚ â€¢ Metadata: {table_name, description, category, ...}   â”‚
â”‚                                                        â”‚
â”‚ Column Collection:                                     â”‚
â”‚ â€¢ ID: "column_sales_amount"                            â”‚
â”‚ â€¢ Document: "Table: sales\nColumn: amount..."          â”‚
â”‚ â€¢ Embedding: [768 floats]                              â”‚
â”‚ â€¢ Metadata: {table_name, column_name, sql_type, ...}   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: Repeat for All Tables                         â”‚
â”‚ â€¢ Process next table                                   â”‚
â”‚ â€¢ Accumulate metadata in ChromaDB                      â”‚
â”‚ â€¢ Log progress                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 9: Analysis Complete                              â”‚
â”‚ print_summary()                                        â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚
â”‚ ğŸ“Š ANALYSIS SUMMARY                                    â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚
â”‚ Tables Analyzed: 3                                     â”‚
â”‚ Total Columns: 45                                      â”‚
â”‚ Vector Database: ./chroma_db_768dim                    â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Time Estimate**: 5-10 minutes for database with 3-5 tables  
**Output**: ChromaDB at `./chroma_db_768dim/` with:
- `table_metadata` collection
- `column_metadata` collection
- All embeddings and metadata

---

### Workflow 2: CSV to Database Conversion

**Script**: `csv_to_db.py`  
**Purpose**: Convert CSV files to SQLite with LLM-inferred schema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: User Runs Converter                            â”‚
â”‚ $ python csv_to_db.py sales.csv --db analysis.db      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Load CSV                                        â”‚
â”‚ df = pd.read_csv("sales.csv")                          â”‚
â”‚ â€¢ Detect encoding automatically                         â”‚
â”‚ â€¢ Parse dates if possible                               â”‚
â”‚ â€¢ Load entire file into memory                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Analyze File Metadata                          â”‚
â”‚ analyze_metadata(df, filename)                         â”‚
â”‚                                                        â”‚
â”‚ LLM Prompt:                                            â”‚
â”‚ - Sheet/table name: "sales"                            â”‚
â”‚ - Columns: "transaction_id, date, amount, category"    â”‚
â”‚ - Sample data (first 5 rows)                           â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ {                                                      â”‚
â”‚   "table_name": "sales_transactions",                  â”‚
â”‚   "description": "E-commerce sales records",           â”‚
â”‚   "category": "sales",                                 â”‚
â”‚   "business_context": "Online store transaction log",  â”‚
â”‚   "suggested_primary_key": "transaction_id",           â”‚
â”‚   "data_quality_notes": ["All dates in YYYY-MM-DD"]   â”‚
â”‚ }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Analyze Each Column                            â”‚
â”‚ FOR col IN df.columns:                                 â”‚
â”‚     analyze_column(col, df[col])                       â”‚
â”‚                                                        â”‚
â”‚ Example for "amount" column:                           â”‚
â”‚ LLM Input:                                             â”‚
â”‚ - Column name: "amount"                                â”‚
â”‚ - Sample values: [45.50, 120.00, 67.25, ...]          â”‚
â”‚ - Unique count: 8,543                                  â”‚
â”‚ - Null count: 0                                        â”‚
â”‚                                                        â”‚
â”‚ LLM Output:                                            â”‚
â”‚ {                                                      â”‚
â”‚   "sql_type": "REAL",                                  â”‚
â”‚   "python_type": "float",                              â”‚
â”‚   "description": "Transaction amount in currency",     â”‚
â”‚   "business_meaning": "Sale price paid by customer",   â”‚
â”‚   "constraints": ["NOT NULL", "CHECK > 0"],            â”‚
â”‚   "is_nullable": false,                                â”‚
â”‚   "suggested_index": false                             â”‚
â”‚ }                                                      â”‚
â”‚                                                        â”‚
â”‚ Fallback: If LLM fails, use pandas type detection      â”‚
â”‚ â€¢ is_integer_dtype â†’ INTEGER                           â”‚
â”‚ â€¢ is_float_dtype â†’ REAL                                â”‚
â”‚ â€¢ is_datetime64_any_dtype â†’ DATE                       â”‚
â”‚ â€¢ else â†’ TEXT                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Create Database Schema                         â”‚
â”‚ create_database(df, metadata, column_analysis)         â”‚
â”‚                                                        â”‚
â”‚ 1. Clean column names:                                 â”‚
â”‚    "Transaction ID" â†’ "transaction_id"                 â”‚
â”‚    "Sale Amount ($)" â†’ "sale_amount"                   â”‚
â”‚                                                        â”‚
â”‚ 2. Build CREATE TABLE DDL:                             â”‚
â”‚    CREATE TABLE IF NOT EXISTS sales_transactions (     â”‚
â”‚        transaction_id TEXT NOT NULL,                   â”‚
â”‚        date DATE,                                      â”‚
â”‚        amount REAL NOT NULL CHECK (amount > 0),        â”‚
â”‚        category TEXT                                   â”‚
â”‚    )                                                   â”‚
â”‚                                                        â”‚
â”‚ 3. Execute DDL to create table                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Type Conversion & Data Insertion               â”‚
â”‚                                                        â”‚
â”‚ Type Conversions:                                      â”‚
â”‚ â€¢ DATE columns â†’ pd.to_datetime()                      â”‚
â”‚ â€¢ INTEGER columns â†’ pd.to_numeric(downcast='integer')  â”‚
â”‚ â€¢ REAL columns â†’ pd.to_numeric()                       â”‚
â”‚                                                        â”‚
â”‚ Bulk Insert:                                           â”‚
â”‚ df_clean.to_sql(                                       â”‚
â”‚     "sales_transactions",                              â”‚
â”‚     conn,                                              â”‚
â”‚     if_exists='replace',                               â”‚
â”‚     index=False                                        â”‚
â”‚ )                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: Verification                                    â”‚
â”‚ SELECT COUNT(*) FROM sales_transactions                â”‚
â”‚ Expected: 10,000                                       â”‚
â”‚ Actual: 10,000 âœ…                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: Success Report                                 â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚
â”‚ âœ… CONVERSION COMPLETE                                 â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚
â”‚ Database: analysis.db                                  â”‚
â”‚ Table: sales_transactions                              â”‚
â”‚ Description: E-commerce sales records                  â”‚
â”‚ Category: sales                                        â”‚
â”‚ Rows: 10,000                                           â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Time Estimate**: 2-5 minutes for 10,000 rows with 10 columns  
**Next Step**: Run `analyze_existing_db.py` to create vector database

---

### Workflow 3: Query Processing (Multi-Turn)

**Primary Script**: QueryAgent interacting through Gradio UI

#### Scenario: New Query (No Context)

**User**: "What are the top 5 sales by amount?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UI: process_question() receives user input            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ agent.answer_question_with_context(question)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Intent Analysis                                â”‚
â”‚ _analyze_question_intent()                            â”‚
â”‚                                                        â”‚
â”‚ Input to LLM:                                          â”‚
â”‚ - Question: "What are the top 5 sales by amount?"      â”‚
â”‚ - Recent conversation: []                              â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ {                                                      â”‚
â”‚   "intent": "new_query",                               â”‚
â”‚   "references_previous": false,                        â”‚
â”‚   "referenced_concepts": ["sales", "amount", "top 5"], â”‚
â”‚   "needs_context": false,                              â”‚
â”‚   "confidence": 0.95                                   â”‚
â”‚ }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Metadata Retrieval                             â”‚
â”‚ _retrieve_metadata(question, top_k=5)                 â”‚
â”‚                                                        â”‚
â”‚ 1. Generate embedding for question:                    â”‚
â”‚    [0.234, -0.567, 0.123, ...] (768 dims)             â”‚
â”‚                                                        â”‚
â”‚ 2. Query ChromaDB table_collection:                    â”‚
â”‚    Similar tables ranked by semantic similarity        â”‚
â”‚    Result: ["sales", "transactions", "revenue"]        â”‚
â”‚                                                        â”‚
â”‚ 3. Query ChromaDB column_collection:                   â”‚
â”‚    Relevant columns: ["amount", "total_sales",         â”‚
â”‚                       "quantity", "price"]             â”‚
â”‚                                                        â”‚
â”‚ 4. Format metadata string:                             â”‚
â”‚    "Table: sales                                       â”‚
â”‚     Description: Transaction records...                â”‚
â”‚     Columns: amount (REAL), date (DATE)..."            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: SQL Generation                                 â”‚
â”‚ _generate_sql_with_retry()                            â”‚
â”‚                                                        â”‚
â”‚ Attempt 1:                                             â”‚
â”‚ LLM Input:                                             â”‚
â”‚ - Database schema (from SQLDatabase)                   â”‚
â”‚ - Metadata context (from ChromaDB)                     â”‚
â”‚ - Question: "What are the top 5 sales by amount?"      â”‚
â”‚                                                        â”‚
â”‚ LLM Output:                                            â”‚
â”‚ ```sql                                                 â”‚
â”‚ SELECT * FROM sales                                    â”‚
â”‚ ORDER BY amount DESC                                   â”‚
â”‚ LIMIT 5                                                â”‚
â”‚ ```                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: SQL Cleaning & Validation                      â”‚
â”‚ _clean_sql() + _validate_and_fix_tables()             â”‚
â”‚                                                        â”‚
â”‚ Cleaning:                                              â”‚
â”‚ â€¢ Remove markdown fences                               â”‚
â”‚ â€¢ Extract SELECT statement                             â”‚
â”‚ â€¢ Balance quotes                                       â”‚
â”‚ â€¢ Fix syntax issues                                    â”‚
â”‚                                                        â”‚
â”‚ Validation:                                            â”‚
â”‚ â€¢ Check table exists: "sales" âœ…                       â”‚
â”‚ â€¢ Check columns exist: "amount" âœ…                     â”‚
â”‚ â€¢ Validate JOINs (if any)                              â”‚
â”‚                                                        â”‚
â”‚ Final SQL:                                             â”‚
â”‚ SELECT * FROM sales                                    â”‚
â”‚ ORDER BY amount DESC                                   â”‚
â”‚ LIMIT 5                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Query Execution                                â”‚
â”‚ pd.read_sql_query(sql, conn)                           â”‚
â”‚                                                        â”‚
â”‚ Result DataFrame:                                      â”‚
â”‚   transaction_id    amount      date        category  â”‚
â”‚ 0    TXN-1234       1536.17   2024-03-15   Electronicsâ”‚
â”‚ 1    TXN-5678       1112.25   2024-02-28   Home       â”‚
â”‚ 2    TXN-9012        765.28   2024-01-12   Sports     â”‚
â”‚ 3    TXN-3456        508.85   2024-04-03   Fashion    â”‚
â”‚ 4    TXN-7890        246.47   2024-03-22   Beauty     â”‚
â”‚                                                        â”‚
â”‚ Shape: (5, 4)                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Answer Generation                              â”‚
â”‚ _generate_answer(question, df, sql)                   â”‚
â”‚                                                        â”‚
â”‚ LLM Prompt:                                            â”‚
â”‚ - Question                                             â”‚
â”‚ - Data summary: "Found 5 rows with columns..."         â”‚
â”‚ - Sample data (first 10 rows)                          â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ "The top 5 sales by amount are:                        â”‚
â”‚  1. $1,536.17 from Electronics (TXN-1234)              â”‚
â”‚  2. $1,112.25 from Home (TXN-5678)                     â”‚
â”‚  3. $765.28 from Sports (TXN-9012)                     â”‚
â”‚  4. $508.85 from Fashion (TXN-3456)                    â”‚
â”‚  5. $246.47 from Beauty (TXN-7890)                     â”‚
â”‚  The highest transaction was in Electronics."          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: Visualization Decision                         â”‚
â”‚ _should_visualize(question, df)                       â”‚
â”‚                                                        â”‚
â”‚ Check keywords: "top", "5", "sales" â†’ No viz keywords â”‚
â”‚ Result: should_visualize = False                       â”‚
â”‚ (User didn't explicitly request visualization)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: State Update                                   â”‚
â”‚ _update_conversation_state()                          â”‚
â”‚                                                        â”‚
â”‚ Add User Message:                                      â”‚
â”‚ Message(role="user",                                   â”‚
â”‚         content="What are the top 5 sales?",           â”‚
â”‚         metadata={"intent": "new_query"})              â”‚
â”‚                                                        â”‚
â”‚ Add Assistant Message:                                 â”‚
â”‚ Message(role="assistant",                              â”‚
â”‚         content="The top 5 sales...",                  â”‚
â”‚         sql_query="SELECT * FROM sales...",            â”‚
â”‚         dataframe_snapshot={                           â”‚
â”‚             "columns": ["transaction_id", ...],        â”‚
â”‚             "row_count": 5,                            â”‚
â”‚             "sample": {first 50 rows as dict}          â”‚
â”‚         },                                             â”‚
â”‚         visualization=None,                            â”‚
â”‚         metadata={"success": True})                    â”‚
â”‚                                                        â”‚
â”‚ Add Data Context:                                      â”‚
â”‚ DataContext(query="SELECT * FROM sales...",            â”‚
â”‚             columns=["transaction_id", "amount", ...], â”‚
â”‚             row_count=5)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 9: UI Display                                     â”‚
â”‚ process_question() returns updated history             â”‚
â”‚                                                        â”‚
â”‚ Gradio Chat Display:                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ â”‚ User: What are the top 5 sales by amount?â”‚          â”‚
â”‚ â”‚                                           â”‚          â”‚
â”‚ â”‚ Assistant: The top 5 sales by amount are:â”‚          â”‚
â”‚ â”‚ 1. $1,536.17 from Electronics...          â”‚          â”‚
â”‚ â”‚ 2. $1,112.25 from Home...                 â”‚          â”‚
â”‚ â”‚ ...                                       â”‚          â”‚
â”‚ â”‚                                           â”‚          â”‚
â”‚ â”‚ [Data Table with 5 rows displayed]        â”‚          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 10: Conversation Persistence                      â”‚
â”‚ save_current_conversation()                           â”‚
â”‚                                                        â”‚
â”‚ File: conversations/a1b2c3d4-uuid.json                 â”‚
â”‚ Content: Full conversation state as JSON               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Scenario: Follow-Up Query with Visualization

**User**: "Show me those sales as a bar chart"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Intent Analysis                                â”‚
â”‚ _analyze_question_intent()                            â”‚
â”‚                                                        â”‚
â”‚ Input to LLM:                                          â”‚
â”‚ - Question: "Show me those sales as a bar chart"       â”‚
â”‚ - Recent conversation:                                 â”‚
â”‚   "- user: What are the top 5 sales by amount?         â”‚
â”‚    - assistant: The top 5 sales are..."                â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ {                                                      â”‚
â”‚   "intent": "re_visualize",                            â”‚
â”‚   "references_previous": true,                         â”‚
â”‚   "referenced_concepts": ["those sales", "bar chart"], â”‚
â”‚   "needs_context": true,                               â”‚
â”‚   "confidence": 0.92                                   â”‚
â”‚ }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Retrieve Previous Data                         â”‚
â”‚ _check_previous_reference()                           â”‚
â”‚                                                        â”‚
â”‚ Keywords detected: "those"                             â”‚
â”‚ â†’ Search conversation_state for latest DataFrame       â”‚
â”‚                                                        â”‚
â”‚ Found: DataFrame from previous query (5 rows)          â”‚
â”‚   transaction_id    amount      date        category  â”‚
â”‚ 0    TXN-1234       1536.17   2024-03-15   Electronicsâ”‚
â”‚ 1    TXN-5678       1112.25   2024-02-28   Home       â”‚
â”‚ ...                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Route to Re-visualization Handler              â”‚
â”‚ _handle_revisualization(question, df)                 â”‚
â”‚                                                        â”‚
â”‚ 1. Extract filters from previous SQL:                  â”‚
â”‚    Previous SQL: SELECT * FROM sales ORDER BY          â”‚
â”‚                  amount DESC LIMIT 5                   â”‚
â”‚    Filters found: None                                 â”‚
â”‚                                                        â”‚
â”‚ 2. Apply filters to DataFrame: N/A                     â”‚
â”‚                                                        â”‚
â”‚ 3. Generate visualization recommendation:              â”‚
â”‚    _should_visualize(question, df)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Visualization Decision                         â”‚
â”‚                                                        â”‚
â”‚ Detect keywords: "bar chart" âœ…                        â”‚
â”‚                                                        â”‚
â”‚ LLM Prompt:                                            â”‚
â”‚ - Question: "Show me those sales as a bar chart"       â”‚
â”‚ - Data info: 5 rows, columns [transaction_id,          â”‚
â”‚               amount, date, category]                  â”‚
â”‚ - Numeric columns: [amount]                            â”‚
â”‚ - Sample data                                          â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ {                                                      â”‚
â”‚   "should_visualize": true,                            â”‚
â”‚   "chart_types": ["bar"],                              â”‚
â”‚   "primary_chart": "bar",                              â”‚
â”‚   "x_axis": "category",                                â”‚
â”‚   "y_axis": "amount",                                  â”‚
â”‚   "color_by": "category",                              â”‚
â”‚   "title": "Top 5 Sales by Amount",                    â”‚
â”‚   "visualization_rationale": "Bar chart effectively    â”‚
â”‚        shows comparison of amounts across categories"  â”‚
â”‚ }                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Create Plotly Chart                            â”‚
â”‚ _create_visualization(df, viz_response)               â”‚
â”‚                                                        â”‚
â”‚ Code executed:                                         â”‚
â”‚ fig = px.bar(                                          â”‚
â”‚     df,                                                â”‚
â”‚     x="category",                                      â”‚
â”‚     y="amount",                                        â”‚
â”‚     color="category",                                  â”‚
â”‚     title="Top 5 Sales by Amount"                      â”‚
â”‚ )                                                      â”‚
â”‚ fig.update_layout(                                     â”‚
â”‚     showlegend=True,                                   â”‚
â”‚     height=500,                                        â”‚
â”‚     xaxis=dict(showgrid=True),                         â”‚
â”‚     yaxis=dict(showgrid=True)                          â”‚
â”‚ )                                                      â”‚
â”‚                                                        â”‚
â”‚ Result: Interactive Plotly Figure object               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Generate Answer                                â”‚
â”‚ _generate_answer(question, filtered_df, previous_sql) â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ "I've created a bar chart showing the top 5 sales      â”‚
â”‚  by amount. Electronics leads with $1,536.17,          â”‚
â”‚  followed by Home at $1,112.25. The visualization      â”‚
â”‚  clearly shows the distribution of high-value          â”‚
â”‚  transactions across different categories."            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: State Update                                   â”‚
â”‚ _update_conversation_state()                          â”‚
â”‚                                                        â”‚
â”‚ Add User Message:                                      â”‚
â”‚ Message(role="user",                                   â”‚
â”‚         content="Show me those sales as a bar chart",  â”‚
â”‚         metadata={"intent": "re_visualize"})           â”‚
â”‚                                                        â”‚
â”‚ Add Assistant Message:                                 â”‚
â”‚ Message(role="assistant",                              â”‚
â”‚         content="I've created a bar chart...",         â”‚
â”‚         sql_query="SELECT * FROM sales..."  (preserved)â”‚
â”‚         dataframe_snapshot={...},                      â”‚
â”‚         visualization="bar",                           â”‚
â”‚         figure_json='{"data": [...], "layout": {...}}',â”‚
â”‚         metadata={"success": True, "reused_data": True})â”‚
â”‚                                                        â”‚
â”‚ Add Visualization Record:                              â”‚
â”‚ VisualizationRecord(                                   â”‚
â”‚     question="Show me those sales as a bar chart",     â”‚
â”‚     chart_type="bar",                                  â”‚
â”‚     data_summary="5 rows"                              â”‚
â”‚ )                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: UI Display                                     â”‚
â”‚                                                        â”‚
â”‚ Gradio Chat Display:                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚ â”‚ User: Show me those sales as a bar chart â”‚          â”‚
â”‚ â”‚                                           â”‚          â”‚
â”‚ â”‚ Assistant: I've created a bar chart...    â”‚          â”‚
â”‚ â”‚                                           â”‚          â”‚
â”‚ â”‚ [Interactive Plotly Bar Chart Displayed]  â”‚          â”‚
â”‚ â”‚  - X-axis: Category                       â”‚          â”‚
â”‚ â”‚  - Y-axis: Amount                         â”‚          â”‚
â”‚ â”‚  - Bars colored by category               â”‚          â”‚
â”‚ â”‚  - Hover tooltips with exact values       â”‚          â”‚
â”‚ â”‚                                           â”‚          â”‚
â”‚ â”‚ [Data Table - same 5 rows]                â”‚          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Scenario: Data Transformation

**User**: "Filter only Electronics category"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Intent Analysis â†’ "transform"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Retrieve Previous DataFrame (5 rows)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Generate Transformation Code                   â”‚
â”‚ _handle_transformation(question, df)                  â”‚
â”‚                                                        â”‚
â”‚ LLM Prompt:                                            â”‚
â”‚ - DataFrame columns: [transaction_id, amount, ...]     â”‚
â”‚ - User request: "Filter only Electronics category"     â”‚
â”‚ - Instructions: Generate pandas code using 'df' var    â”‚
â”‚                                                        â”‚
â”‚ LLM Response:                                          â”‚
â”‚ df[df['category'] == 'Electronics']                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Execute Transformation Safely                  â”‚
â”‚                                                        â”‚
â”‚ local_vars = {"df": df.copy(), "pd": pd}               â”‚
â”‚ exec(code, {"__builtins__": {}}, local_vars)           â”‚
â”‚ transformed_df = local_vars["df"]                      â”‚
â”‚                                                        â”‚
â”‚ Result:                                                â”‚
â”‚   transaction_id    amount      date        category  â”‚
â”‚ 0    TXN-1234       1536.17   2024-03-15   Electronicsâ”‚
â”‚                                                        â”‚
â”‚ Shape: (1, 4)                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Generate Answer                                â”‚
â”‚ "I've filtered the data to show only Electronics       â”‚
â”‚  category. Found 1 transaction with amount $1,536.17." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: State Update & UI Display                      â”‚
â”‚ (Similar pattern as previous scenarios)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CONVERSATION MANAGEMENT SYSTEM

### State Persistence Architecture

```
Memory (Runtime)                    Disk (Persistent)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ConversationStateâ”‚               â”‚ conversations/     â”‚
â”‚ â”œâ”€ messages[]    â”‚  â”€â”€save()â”€â”€>  â”‚ â”œâ”€ uuid1.json      â”‚
â”‚ â”œâ”€ data_contexts[]â”‚               â”‚ â”œâ”€ uuid2.json      â”‚
â”‚ â””â”€ visualizations[]â”‚              â”‚ â””â”€ uuid3.json      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€load()â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Message Flow Timeline

```
Time: T0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: "What are top sales?"
  â†“
[Intent: NEW_QUERY] â†’ Generate SQL â†’ Execute
  â†“
conversation_state.add_message(user_msg)
conversation_state.add_message(assistant_msg with SQL & snapshot)
conversation_state.add_data_context(DataContext)
  â†“
save_current_conversation()
  â†“
File: conversations/a1b2c3d4.json created
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time: T1 (5 seconds later)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: "Show as pie chart"
  â†“
[Intent: RE_VISUALIZE] â†’ Retrieve previous DF
  â†“
conversation_state.add_message(user_msg)
conversation_state.add_message(assistant_msg with figure_json)
conversation_state.add_visualization(VisualizationRecord)
  â†“
save_current_conversation()
  â†“
File: conversations/a1b2c3d4.json updated
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time: T2 (Next day)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: Clicks on conversation in sidebar
  â†“
load_conversation("a1b2c3d4.json")
  â†“
conversation_state.import_conversation(data)
  â†“
Reconstruct full history with charts & tables
  â†“
Display in UI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Memory Management Strategy

**Problem**: Long conversations can consume excessive memory.

**Solution**: Multi-level cleanup

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Existing DB    â”‚
â”‚  (analysis.db)  â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Table 1    â”‚ â”‚
â”‚  â”‚ Table 2    â”‚ â”‚
â”‚  â”‚ Table 3    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ python analyze_existing_db.py analysis.db
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  analyze_existing_db.py         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Read table structure         â”‚
â”‚  â€¢ Sample data                  â”‚
â”‚  â€¢ Call Ollama LLM              â”‚
â”‚  â€¢ Analyze metadata             â”‚
â”‚  â€¢ Analyze each column          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Ollama LLM     â”‚
    â”‚   (qwen2.5:7b)     â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
    â”‚  Understanding:  â”‚
    â”‚  â€¢ Business      â”‚
    â”‚    context       â”‚
    â”‚  â€¢ Data types    â”‚
    â”‚  â€¢ Constraints   â”‚
    â”‚  â€¢ Relationships â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  metadata.db (Generated)      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                               â”‚
â”‚  table_metadata               â”‚
â”‚  â”œâ”€ table_name                â”‚
â”‚  â”œâ”€ description               â”‚
â”‚  â”œâ”€ category                  â”‚
â”‚  â”œâ”€ business_context          â”‚
â”‚  â”œâ”€ suggested_primary_key     â”‚
â”‚  â””â”€ data_quality_notes        â”‚
â”‚                               â”‚
â”‚  column_metadata              â”‚
â”‚  â”œâ”€ column_name               â”‚
â”‚  â”œâ”€ sql_type                  â”‚
â”‚  â”œâ”€ python_type               â”‚
â”‚  â”œâ”€ description               â”‚
â”‚  â”œâ”€ business_meaning          â”‚
â”‚  â”œâ”€ constraints               â”‚
â”‚  â””â”€ statistics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ âš ï¸ ONE-TIME PROCESS
            â”‚ Re-run only when schema changes
            â”‚
            â†“

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    PHASE 2: QUERY & VISUALIZATION                         â•‘
â•‘                         (Fast, Anytime)                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Question                â”‚
â”‚  (Natural Language)           â”‚
â”‚                               â”‚
â”‚  "What are the top 10 sales?" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ python main_query_agent.py --interactive
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  QueryAgent_Ollama.py                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  1. Load metadata from metadata.db     â”‚
â”‚  2. Connect to source DB               â”‚
â”‚  3. Send question to Ollama            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Ollama LLM     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”¤  Metadata Context  â”‚
    â”‚   (qwen2.5:7b)     â”‚        â”‚  â€¢ Table meanings  â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚        â”‚  â€¢ Column types    â”‚
    â”‚  Generates:      â”‚        â”‚  â€¢ Business rules  â”‚
    â”‚  â€¢ SQL Query     â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚  â€¢ Answer text   â”‚
    â”‚  â€¢ Viz strategy  â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQL Query Execution          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  SELECT * FROM sales          â”‚
â”‚  ORDER BY amount DESC         â”‚
â”‚  LIMIT 10                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  analysis.db                  â”‚
â”‚  (Source Database)            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Execute query                â”‚
â”‚  Return results               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Results Processing                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  1. Format results as DataFrame        â”‚
â”‚  2. Generate detailed answer (LLM)     â”‚
â”‚  3. Check if visualization needed      â”‚
â”‚  4. Create Plotly chart                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output to User                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  ğŸ“Š DETAILED ANSWER                    â”‚
â”‚  â”œâ”€ Natural language explanation       â”‚
â”‚  â”œâ”€ Key findings                       â”‚
â”‚  â””â”€ Specific numbers                   â”‚
â”‚                                        â”‚
â”‚  ğŸ” SQL QUERY USED                     â”‚
â”‚  â””â”€ Formatted SQL                      â”‚
â”‚                                        â”‚
â”‚  ğŸ“‹ QUERY RESULTS                      â”‚
â”‚  â””â”€ Data table                         â”‚
â”‚                                        â”‚
â”‚  ğŸ“Š VISUALIZATION (if requested)       â”‚
â”‚  â”œâ”€ Interactive Plotly chart           â”‚
â”‚  â””â”€ Export to HTML                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow Comparison

### Old System (CSV/Excel â†’ Analysis â†’ Query)
```
â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ CSV  â”‚â”€â”€â†’â”‚ Analyze â”‚â”€â”€â†’â”‚  DB  â”‚â”€â”€â†’â”‚ Query â”‚
â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜
           (Every time)              (Every time)
```

### New System (Pre-analyzed â†’ Query)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Existing â”‚â”€â”€â†’â”‚ Analyze â”‚â”€â”€â†’â”‚ Metadata â”‚
â”‚    DB    â”‚   â”‚ (Once)  â”‚   â”‚    DB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â”‚ Fast lookup
                                   â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Query  â”‚
                              â”‚ (Fast!) â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Interaction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Ollama LLM Server                      â”‚
â”‚                   (http://localhost:11434)                  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   qwen2.5:7b  â”‚  â”‚qwen2.5:7bb   â”‚  â”‚ Other Models â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ HTTP API calls
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                     â”‚
        â†“                                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analyze_existing  â”‚              â”‚  QueryAgent_Ollama  â”‚
â”‚     _db.py        â”‚              â”‚       .py           â”‚
â”‚                   â”‚              â”‚                     â”‚
â”‚ One-time analysis â”‚              â”‚  Query execution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
         â”‚ Writes                             â”‚ Reads
         â†“                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ metadata.dbâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ metadata.dbâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    Metadata lookup   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
         â”‚ Reads                              â”‚ Queries
         â†“                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚analysis.db â”‚                      â”‚analysis.db â”‚
    â”‚(Structure) â”‚                      â”‚  (Data)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Workflow Timeline

```
Time: T0 (Initial Setup)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”‚  [User runs analyzer]
â”‚   python analyze_existing_db.py analysis.db
â”‚
â”œâ”€â†’ ğŸ” Read database structure
â”œâ”€â†’ ğŸ¤– LLM analyzes tables/columns (5-10 min)
â”œâ”€â†’ ğŸ’¾ Save to metadata.db
â”‚
â”‚  âœ… Setup complete!
â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Time: T1, T2, T3... (Subsequent queries)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚
â”‚  [User asks question]
â”‚   python main_query_agent.py --interactive
â”‚
â”œâ”€â†’ âš¡ Load metadata (instant)
â”œâ”€â†’ ğŸ¤– Generate SQL (2-3 sec)
â”œâ”€â†’ ğŸ“Š Execute & visualize (1-2 sec)
â”‚
â”‚  âœ… Fast response!
â”‚
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## File Dependencies

```
analyze_existing_db.py
    â”‚
    â”œâ”€â”€ pandas
    â”œâ”€â”€ sqlite3
    â”œâ”€â”€ langchain_ollama (ChatOllama)
    â”œâ”€â”€ langchain_core (PromptTemplate, StrOutputParser)
    â”œâ”€â”€ pydantic (BaseModel, Field)
    â””â”€â”€ langchain_core.output_parsers (PydanticOutputParser)

QueryAgent_Ollama.py
    â”‚
    â”œâ”€â”€ pandas
    â”œâ”€â”€ sqlite3
    â”œâ”€â”€ plotly (express, graph_objects)
    â”œâ”€â”€ langchain_ollama (ChatOllama)
    â”œâ”€â”€ langchain_core (PromptTemplate, StrOutputParser)
    â”œâ”€â”€ langchain_community (SQLDatabase, QuerySQLDatabaseTool)
    â”œâ”€â”€ langchain_classic (create_sql_query_chain)
    â””â”€â”€ pydantic (BaseModel, Field)

main_query_agent.py
    â”‚
    â”œâ”€â”€ argparse
    â”œâ”€â”€ json
    â”œâ”€â”€ os
    â”œâ”€â”€ QueryAgent_Ollama (QueryAgent)
    â””â”€â”€ plotly.graph_objects (for viz export)
```

## Database Schema

```
metadata.db
â”‚
â”œâ”€â”€ table_metadata
â”‚   â”œâ”€â”€ table_name (PK)
â”‚   â”œâ”€â”€ description
â”‚   â”œâ”€â”€ category
â”‚   â”œâ”€â”€ business_context
â”‚   â”œâ”€â”€ suggested_primary_key
â”‚   â”œâ”€â”€ data_quality_notes (JSON)
â”‚   â”œâ”€â”€ row_count
â”‚   â”œâ”€â”€ column_count
â”‚   â””â”€â”€ analyzed_at
â”‚
â””â”€â”€ column_metadata
    â”œâ”€â”€ id (PK)
    â”œâ”€â”€ table_name (FK)
    â”œâ”€â”€ column_name
    â”œâ”€â”€ sql_type
    â”œâ”€â”€ python_type
    â”œâ”€â”€ description
    â”œâ”€â”€ business_meaning
    â”œâ”€â”€ constraints (JSON)
    â”œâ”€â”€ is_nullable
    â”œâ”€â”€ suggested_index
    â”œâ”€â”€ unique_count
    â”œâ”€â”€ null_count
    â””â”€â”€ analyzed_at
```

## Performance Characteristics

```
Operation                   Old System    New System
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Initial Setup               N/A           5-10 min
Subsequent Startup          30-60 sec     <1 sec
Query Generation            3-5 sec       3-5 sec
Total Response Time         33-65 sec     3-6 sec
Memory Usage               High          Low
Reusability                No            Yes
```

## Error Handling Flow

```
User Question
    â”‚
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Ollama    â”‚â”€â”€â†’ Not running? â†’ Return error
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Running
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate SQL   â”‚â”€â”€â†’ Failed? â†’ Return error with details
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Success
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Query  â”‚â”€â”€â†’ SQL error? â†’ Log & return friendly error
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Success
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate Answerâ”‚â”€â”€â†’ Failed? â†’ Return raw results
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Success
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create Viz     â”‚â”€â”€â†’ Failed? â†’ Skip viz, return answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Success
         â†“
  Return Complete Results
```

---

## COMPLETE DEPLOYMENT GUIDE - FROM ZERO TO PRODUCTION

This section provides a comprehensive, step-by-step guide to deploy the Conversational Data Analytics System on a completely new machine from scratch to getting your first answer.

### Prerequisites Check

Before starting, verify you have:

**Hardware Requirements**:
- **CPU**: Multi-core processor (4+ cores recommended)
- **RAM**: Minimum 8 GB, 16 GB recommended for production
- **Storage**: At least 10 GB free space for models and data
- **GPU** (Optional): NVIDIA GPU with CUDA for faster inference

**Operating Systems Supported**:
- âœ… Windows 10/11 (64-bit)
- âœ… Linux (Ubuntu 20.04+, Debian 11+, RHEL 8+)
- âœ… macOS 11+ (Big Sur or later)

### Phase 1: Environment Setup (15-20 minutes)

#### Step 1.1: Install Python 3.10+

**Windows**:
```powershell
# Download from python.org or use winget
winget install Python.Python.3.10

# Verify installation
python --version
# Expected: Python 3.10.x or higher
```

**Linux (Ubuntu/Debian)**:
```bash
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip -y
python3.10 --version
```

**macOS**:
```bash
# Using Homebrew
brew install python@3.10
python3.10 --version
```

#### Step 1.2: Install Ollama

**Windows**:
1. Download installer from: https://ollama.com/download/windows
2. Run `OllamaSetup.exe`
3. Follow installation wizard
4. Ollama will start automatically

**Linux**:
```bash
# One-line install script
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version
# Expected: ollama version is x.x.x
```

**macOS**:
```bash
# Download from ollama.com/download/mac or use Homebrew
brew install ollama

# Start Ollama service
ollama serve &
```

#### Step 1.3: Verify Ollama is Running

```powershell
# Test Ollama API
curl http://localhost:11434/api/tags

# Expected response (JSON with available models)
# {"models":[]}  # Empty initially, we'll add models next
```

**Troubleshooting**:
- **Connection refused**: Start Ollama manually with `ollama serve`
- **Port 11434 in use**: Another process is using the port, restart Ollama
- **Firewall blocking**: Allow Ollama through firewall

#### Step 1.4: Download Required Models

```powershell
# Pull main LLM model (qwen2.5:7b - approximately 4.7 GB)
ollama pull qwen2.5:7b

# Expected output:
# pulling manifest
# pulling <hash>... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.7 GB
# pulling <hash>... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 1.5 KB
# pulling <hash>... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 6.9 KB
# verifying sha256 digest
# writing manifest
# success

# Pull embedding model (nomic-embed-text - approximately 275 MB)
ollama pull nomic-embed-text

# Expected output:
# pulling manifest
# pulling <hash>... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 274 MB
# ...
# success
```

**Verify Models**:
```powershell
ollama list

# Expected output:
# NAME                    ID              SIZE      MODIFIED
# qwen2.5:7b              abc123...       4.7 GB    2 minutes ago
# nomic-embed-text        def456...       274 MB    1 minute ago
```

**Time**: ~10-15 minutes depending on internet speed

---

### Phase 2: Project Setup (5-10 minutes)

#### Step 2.1: Get the Project Files

**Option A: Clone from Repository** (if available):
```powershell
git clone <repository-url>
cd "Ollama 2"
```

**Option B: Manual Setup** (if files provided separately):
```powershell
# Create project directory
mkdir "Ollama 2"
cd "Ollama 2"

# Copy provided files:
# - app_gradio_enhanced.py
# - QueryAgent_Ollama_Enhanced.py
# - conversation_manager.py
# - requirements.txt
# - PRE PROCESSING/ folder with scripts
```

#### Step 2.2: Create Virtual Environment

```powershell
# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
.\venv\Scripts\activate

# Linux/macOS:
# source venv/bin/activate

# Verify activation (prompt should show (venv))
# (venv) PS C:\Users\...\Ollama 2>
```

#### Step 2.3: Install Python Dependencies

```powershell
# Install all required packages
pip install -r requirements.txt

# Expected packages:
# pandas==2.1.3
# plotly==5.18.0
# gradio==4.8.0
# langchain==0.1.0
# langchain-community==0.0.10
# langchain-ollama==0.0.3
# chromadb==0.4.18
# pydantic==2.5.2
# python-dotenv==1.0.0
# requests==2.31.0

# Verify installation
pip list | Select-String "gradio|langchain|chromadb|plotly"

# Expected output:
# chromadb                 0.4.18
# gradio                   4.8.0
# langchain                0.1.0
# langchain-community      0.0.10
# langchain-ollama         0.0.3
# plotly                   5.18.0
```

**If Installation Fails**:
```powershell
# Upgrade pip first
python -m pip install --upgrade pip

# Try installing packages individually
pip install pandas plotly gradio
pip install langchain langchain-community langchain-ollama
pip install chromadb pydantic requests
```

#### Step 2.4: Verify Project Structure

```powershell
# Check directory structure
tree /F /A

# Expected structure:
# Ollama 2/
# â”œâ”€â”€ app_gradio_enhanced.py
# â”œâ”€â”€ QueryAgent_Ollama_Enhanced.py
# â”œâ”€â”€ conversation_manager.py
# â”œâ”€â”€ requirements.txt
# â”œâ”€â”€ PRE PROCESSING/
# â”‚   â”œâ”€â”€ analyze_existing_db.py
# â”‚   â”œâ”€â”€ csv_to_db.py
# â”‚   â”œâ”€â”€ ARCHITECTURE.md
# â”‚   â””â”€â”€ ...
# â””â”€â”€ venv/
```

---

### Phase 3: Data Preparation (10-30 minutes depending on data size)

#### Step 3.1: Prepare Your Database

**Option A: You Have an Existing SQLite Database**

```powershell
# Copy your database to project root
Copy-Item "C:\path\to\your\database.db" -Destination ".\analysis.db"

# Verify database is readable
sqlite3 analysis.db ".tables"
# Expected: List of your tables
```

**Option B: You Have CSV Files**

```powershell
# Place CSV file in project directory
Copy-Item "C:\path\to\your\data.csv" -Destination ".\data.csv"

# Convert CSV to SQLite using our script
python "PRE PROCESSING/csv_to_db.py" data.csv --db analysis.db

# Script will:
# 1. Load CSV
# 2. Ask LLM to analyze structure
# 3. Infer column types
# 4. Create optimized database schema
# 5. Insert all data
```

**Example Output**:
```
ğŸ”„ CSV TO DATABASE CONVERTER
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Input CSV: data.csv
Output Database: analysis.db
LLM Model: qwen2.5:7b
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Loading CSV...
   Detected: 10,000 rows, 8 columns

ğŸ§  Analyzing metadata with LLM...
   Table name: sales_transactions
   Category: sales
   Description: E-commerce sales data

ğŸ” Analyzing columns...
   [1/8] transaction_id â†’ TEXT (unique identifier)
   [2/8] date â†’ DATE (transaction timestamp)
   [3/8] amount â†’ REAL (sale amount)
   [4/8] category â†’ TEXT (product category)
   [5/8] customer_id â†’ TEXT (customer identifier)
   [6/8] product_name â†’ TEXT (product description)
   [7/8] quantity â†’ INTEGER (items sold)
   [8/8] payment_method â†’ TEXT (payment type)

ğŸ’¾ Creating database schema...
   CREATE TABLE sales_transactions (
       transaction_id TEXT NOT NULL,
       date DATE,
       amount REAL NOT NULL CHECK (amount > 0),
       category TEXT,
       customer_id TEXT,
       product_name TEXT,
       quantity INTEGER,
       payment_method TEXT
   )

ğŸ“¥ Inserting data...
   Progress: 10,000/10,000 rows

âœ… CONVERSION COMPLETE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Database: analysis.db
Table: sales_transactions
Rows: 10,000
Time: 4m 32s
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Time**: 2-5 minutes for small datasets (<50K rows), 10-30 minutes for larger datasets

#### Step 3.2: Analyze Database (Create Vector Database)

This is a **critical one-time step** that creates the intelligence layer for metadata-driven SQL generation.

```powershell
# Run the analyzer
python "PRE PROCESSING/analyze_existing_db.py" analysis.db

# What this does:
# 1. Connects to your SQLite database
# 2. For each table:
#    - Asks LLM to analyze table purpose
#    - Asks LLM to analyze each column's meaning
#    - Generates semantic embeddings (768-dim vectors)
#    - Stores everything in ChromaDB
# 3. Creates ./chroma_db_768dim/ directory
```

**Example Output**:
```
ğŸ” DATABASE ANALYZER (Vector DB Edition)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source Database: analysis.db
Vector Database: ./chroma_db_768dim
LLM Model: qwen2.5:7b
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Discovering tables...
   Found 3 table(s): ['sales_transactions', 'products', 'customers']

[1/3] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” Analyzing table: sales_transactions
   Rows: 10,000
   Columns: 8

   ğŸ§  Table Analysis:
      Description: E-commerce transaction records with sales data
      Category: sales
      Business Context: Core revenue tracking system
      Primary Key: transaction_id
      Data Quality: Good - minimal nulls, consistent formatting

   ğŸ“‹ Column Analysis:
      [1/8] transaction_id
            Type: TEXT â†’ Unique transaction identifier
            Business Meaning: Order tracking number
            âœ… Embedding generated (768-dim)

      [2/8] date
            Type: DATE â†’ Transaction timestamp
            Business Meaning: When sale occurred
            âœ… Embedding generated (768-dim)

      [3/8] amount
            Type: REAL â†’ Sale total in USD
            Business Meaning: Revenue per transaction
            âœ… Embedding generated (768-dim)

      [4/8] category
            Type: TEXT â†’ Product classification
            Business Meaning: Grouping for analytics
            âœ… Embedding generated (768-dim)

      [5/8] customer_id
            Type: TEXT â†’ Customer identifier
            Business Meaning: Links to customer records
            âœ… Embedding generated (768-dim)

      [6/8] product_name
            Type: TEXT â†’ Product description
            Business Meaning: What was sold
            âœ… Embedding generated (768-dim)

      [7/8] quantity
            Type: INTEGER â†’ Number of items
            Business Meaning: Order quantity
            âœ… Embedding generated (768-dim)

      [8/8] payment_method
            Type: TEXT â†’ How customer paid
            Business Meaning: Payment channel tracking
            âœ… Embedding generated (768-dim)

âœ… Successfully analyzed and saved: sales_transactions

[2/3] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” Analyzing table: products
   ...
   (similar detailed output)

[3/3] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” Analyzing table: customers
   ...

ğŸ‰ Analysis complete!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š ANALYSIS SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Tables Analyzed: 3
Total Columns: 24
Vector Database: ./chroma_db_768dim
Collections Created:
  â€¢ table_metadata (3 documents)
  â€¢ column_metadata (24 documents)
Total Embeddings: 27
Storage Size: ~2.1 MB
Time: 8m 15s
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ¨ Next Step: Launch the UI with:
   python app_gradio_enhanced.py
```

**Time**: 5-10 minutes for 3-5 tables with 30-50 total columns

**What Just Happened**:
- Created `./chroma_db_768dim/` directory with ChromaDB collections
- Each table has semantic metadata (description, category, context)
- Each column has detailed metadata (type, meaning, constraints)
- All metadata is searchable via semantic similarity
- This enables intelligent SQL generation without manually writing schemas

**Verify Vector Database**:
```powershell
# Check directory exists
Test-Path ".\chroma_db_768dim"
# Expected: True

# Check size
(Get-ChildItem ".\chroma_db_768dim" -Recurse | Measure-Object -Property Length -Sum).Sum / 1MB
# Expected: ~2-5 MB depending on data
```

---

### Phase 4: Launch the Application (1-2 minutes)

#### Step 4.1: Start the Gradio UI

```powershell
# Ensure virtual environment is activated
# (venv) should be in your prompt

# Launch the application
python app_gradio_enhanced.py
```

**Expected Output**:
```
ğŸš€ Starting Conversational Data Analytics System...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Conversations directory: .\conversations
   Status: âœ… Ready

ğŸŒ Launching Gradio interface...

Running on local URL:  http://0.0.0.0:6969

To create a public link, set `share=True` in `launch()`.
```

**What This Means**:
- Web server started on port 6969
- Accessible from any device on your local network
- `0.0.0.0` means listening on all network interfaces

#### Step 4.2: Open Web Browser

```powershell
# Open browser automatically
Start-Process "http://localhost:6969"

# Or manually navigate to:
# http://localhost:6969
# or
# http://127.0.0.1:6969
```

**You Should See**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  ğŸ—„ï¸ Chat History          â”‚  ğŸ’¬ Conversational Data Analytics  â”‚
â”‚                            â”‚                                    â”‚
â”‚  [+ New Chat] [ğŸ—‘ï¸]        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                            â”‚  â”‚                              â”‚  â”‚
â”‚  Recent Conversations:     â”‚  â”‚     (Empty chat area)        â”‚  â”‚
â”‚  (None yet)                â”‚  â”‚                              â”‚  â”‚
â”‚                            â”‚  â”‚                              â”‚  â”‚
â”‚  âš™ï¸ Settings               â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Database Path:            â”‚                                    â”‚
â”‚  [analysis.db        ]     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                            â”‚  â”‚ Ask a question...            â”‚â¤â”‚
â”‚  Vector DB Path:           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  [./chroma_db_768dim ]     â”‚                                    â”‚
â”‚                            â”‚                                    â”‚
â”‚  Model:                    â”‚                                    â”‚
â”‚  [qwen2.5:7b        â–¼]     â”‚                                    â”‚
â”‚                            â”‚                                    â”‚
â”‚  [Initialize LLM Agent]    â”‚                                    â”‚
â”‚                            â”‚                                    â”‚
â”‚  Status: Not Connected     â”‚                                    â”‚
â”‚                            â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Step 4.3: Initialize the Agent

**Steps in UI**:

1. **Verify Settings** (should be pre-filled):
   - Database Path: `analysis.db`
   - Vector DB Path: `./chroma_db_768dim`
   - Model: `qwen2.5:7b`

2. **Click "Initialize the LLM Agent"**

**What Happens**:
```
Initializing...
â”œâ”€ Checking database: analysis.db âœ…
â”œâ”€ Connecting to SQLite... âœ…
â”œâ”€ Loading ChromaDB collections... âœ…
â”‚  â”œâ”€ table_metadata (3 documents) âœ…
â”‚  â””â”€ column_metadata (24 documents) âœ…
â”œâ”€ Connecting to Ollama (localhost:11434)... âœ…
â”œâ”€ Loading model: qwen2.5:7b... âœ…
â”œâ”€ Creating conversation state... âœ…
â””â”€ Agent ready!
```

**Status Changes To**:
```
Status: âœ… Agent Ready
Database: analysis.db (3 tables, 24 columns)
Model: qwen2.5:7b
```

**Troubleshooting**:
- **"Database not found"**: Check path, use absolute path if needed
- **"Vector database not found"**: Run `analyze_existing_db.py` first
- **"Cannot connect to Ollama"**: Ensure `ollama serve` is running
- **"Model not found"**: Run `ollama pull qwen2.5:7b`

---

### Phase 5: Verification Test (2-3 minutes)

Let's test the system with a simple query to ensure everything works.

#### Test 1: Simple Query

**In the chat input, type**:
```
What tables are available in the database?
```

**Click â¤ or press Enter**

**Expected Response** (appears in ~3-5 seconds):
```
Assistant:
The database contains the following tables:

1. **sales_transactions** (10,000 rows)
   - E-commerce transaction records with sales data
   - Columns: transaction_id, date, amount, category, customer_id, 
     product_name, quantity, payment_method

2. **products** (500 rows)
   - Product catalog with details
   - Columns: product_id, name, category, price, stock_quantity

3. **customers** (2,500 rows)
   - Customer information records
   - Columns: customer_id, name, email, join_date, loyalty_tier

You can ask me questions about any of these tables!
```

**What Just Happened**:
1. âœ… UI sent question to agent
2. âœ… Agent analyzed intent (NEW_QUERY)
3. âœ… Agent retrieved metadata from ChromaDB
4. âœ… LLM generated natural language answer
5. âœ… Answer displayed in chat
6. âœ… Conversation auto-saved to `conversations/<uuid>.json`

#### Test 2: Data Query with Visualization

**Type**:
```
Show me the top 5 sales by amount as a bar chart
```

**Expected Response** (~4-6 seconds):
```
Assistant:
Here are the top 5 sales by amount:

1. $1,536.17 - Electronics (TXN-1234) on 2024-03-15
2. $1,112.25 - Home (TXN-5678) on 2024-02-28
3. $765.28 - Sports (TXN-9012) on 2024-01-12
4. $508.85 - Fashion (TXN-3456) on 2024-04-03
5. $246.47 - Beauty (TXN-7890) on 2024-03-22

I've created a bar chart showing these top sales by amount.

[Interactive Plotly Bar Chart Appears]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top 5 Sales by Amount                 â”‚
â”‚                                        â”‚
â”‚  1,600â”‚                                â”‚
â”‚  1,400â”‚     â–ˆâ–ˆâ–ˆ                        â”‚
â”‚  1,200â”‚     â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ                   â”‚
â”‚  1,000â”‚     â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ                   â”‚
â”‚    800â”‚     â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ              â”‚
â”‚    600â”‚     â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ         â”‚
â”‚    400â”‚     â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ    â”‚
â”‚    200â”‚     â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆ    â”‚
â”‚      0â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚        Elec Home Sport Fash Beau       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Data Table Appears]
  transaction_id   amount      date       category
0     TXN-1234     1536.17   2024-03-15  Electronics
1     TXN-5678     1112.25   2024-02-28  Home
2     TXN-9012      765.28   2024-01-12  Sports
3     TXN-3456      508.85   2024-04-03  Fashion
4     TXN-7890      246.47   2024-03-22  Beauty
```

**What Just Happened**:
1. âœ… Intent detected: NEW_QUERY + visualization requested
2. âœ… Semantic search found "sales_transactions" table
3. âœ… SQL generated: `SELECT * FROM sales_transactions ORDER BY amount DESC LIMIT 5`
4. âœ… SQL validated and executed
5. âœ… Natural language answer generated
6. âœ… Visualization recommendation: bar chart
7. âœ… Plotly bar chart created and displayed
8. âœ… Data table rendered
9. âœ… Conversation saved with all data

#### Test 3: Follow-Up Question (Context Test)

**Type**:
```
Show those as a pie chart instead
```

**Expected Response** (~2-3 seconds, faster than first query):
```
Assistant:
I've converted the top 5 sales data into a pie chart showing the 
proportion of sales amount by category.

[Interactive Plotly Pie Chart Appears]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top 5 Sales Distribution by Amount    â”‚
â”‚                                        â”‚
â”‚          â•±â”€â”€â”€â”€â”€â”€â”€â•²                     â”‚
â”‚      â•±â”€â”€â”€  37.5%  â”€â”€â”€â•²                 â”‚
â”‚    â•±   Electronics    â•²                â”‚
â”‚   â”‚                    â”‚               â”‚
â”‚   â”‚  27.2%            â”‚ 18.7%          â”‚
â”‚   â”‚  Home              Sports          â”‚
â”‚    â•²                  â•±                â”‚
â”‚      â•²â”€â”€â”€ 12.4% â”€â”€â”€â•±                   â”‚
â”‚         Fashion                        â”‚
â”‚           6.2% Beauty                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Same Data Table]
```

**What Just Happened**:
1. âœ… Intent detected: RE_VISUALIZE (context-aware)
2. âœ… Referenced "those" â†’ retrieved previous DataFrame from memory
3. âœ… **No new SQL query** (reused existing data)
4. âœ… Created new pie chart visualization
5. âœ… Displayed with same data
6. âœ… Faster response (skipped SQL generation)

**ğŸ‰ SUCCESS! Your system is fully operational!**

---

### Phase 6: Production Deployment (Optional)

#### Option A: Run as Background Service (Windows)

**Create PowerShell Script** (`start-analytics.ps1`):
```powershell
# Start Ollama if not running
$ollama = Get-Process ollama -ErrorAction SilentlyContinue
if (-not $ollama) {
    Start-Process "ollama" -ArgumentList "serve" -WindowStyle Hidden
    Start-Sleep -Seconds 3
}

# Activate virtual environment and start app
cd "C:\path\to\Ollama 2"
.\venv\Scripts\activate
python app_gradio_enhanced.py
```

**Create Scheduled Task**:
```powershell
# Run on system startup
$action = New-ScheduledTaskAction -Execute "PowerShell.exe" `
    -Argument "-File C:\path\to\start-analytics.ps1"

$trigger = New-ScheduledTaskTrigger -AtStartup

Register-ScheduledTask -TaskName "DataAnalyticsAI" `
    -Action $action -Trigger $trigger -RunLevel Highest
```

#### Option B: Docker Deployment

**Create Dockerfile**:
```dockerfile
FROM python:3.10-slim

# Install system dependencies
RUN apt-get update && apt-get install -y curl && \
    curl -fsSL https://ollama.com/install.sh | sh

# Set working directory
WORKDIR /app

# Copy application files
COPY . /app

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Pull Ollama models
RUN ollama serve & \
    sleep 10 && \
    ollama pull qwen2.5:7b && \
    ollama pull nomic-embed-text

# Expose Gradio port
EXPOSE 6969

# Create startup script
RUN echo '#!/bin/bash\nollama serve &\nsleep 5\npython app_gradio_enhanced.py' > /app/start.sh && \
    chmod +x /app/start.sh

# Run application
CMD ["/app/start.sh"]
```

**Build and Run**:
```powershell
# Build image
docker build -t data-analytics-ai .

# Run container
docker run -d `
    -p 6969:6969 `
    -v ${PWD}/conversations:/app/conversations `
    -v ${PWD}/analysis.db:/app/analysis.db `
    -v ${PWD}/chroma_db_768dim:/app/chroma_db_768dim `
    --name analytics-system `
    data-analytics-ai

# View logs
docker logs -f analytics-system

# Access at http://localhost:6969
```

#### Option C: Linux systemd Service

**Create service file** (`/etc/systemd/system/data-analytics.service`):
```ini
[Unit]
Description=Conversational Data Analytics AI System
After=network.target

[Service]
Type=simple
User=youruser
WorkingDirectory=/home/youruser/Ollama 2
Environment="PATH=/home/youruser/Ollama 2/venv/bin:/usr/local/bin:/usr/bin:/bin"
ExecStartPre=/usr/local/bin/ollama serve &
ExecStart=/home/youruser/Ollama 2/venv/bin/python app_gradio_enhanced.py
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

**Enable and start**:
```bash
sudo systemctl daemon-reload
sudo systemctl enable data-analytics
sudo systemctl start data-analytics
sudo systemctl status data-analytics

# View logs
sudo journalctl -u data-analytics -f
```

---

## END-TO-END USER GUIDE - GETTING FINAL ANSWERS

This section demonstrates how to interact with the system to get answers, from simple queries to complex multi-turn conversations with visualizations.

### Understanding the Interface

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                    â”‚
â”‚  SIDEBAR (Left)              â”‚  MAIN CHAT AREA (Right)            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚                               â”‚                                    â”‚
â”‚  ğŸ—„ï¸ Chat History             â”‚  ğŸ’¬ Conversation Display           â”‚
â”‚  â€¢ New Chat button           â”‚  â€¢ User messages (right-aligned)   â”‚
â”‚  â€¢ Delete button             â”‚  â€¢ Assistant messages (left)       â”‚
â”‚  â€¢ List of saved chats       â”‚  â€¢ Charts (interactive Plotly)     â”‚
â”‚                               â”‚  â€¢ Data tables (HTML)              â”‚
â”‚  âš™ï¸ Settings                  â”‚                                    â”‚
â”‚  â€¢ Database path             â”‚  ğŸ“ Input Area                     â”‚
â”‚  â€¢ Vector DB path            â”‚  â€¢ Text input box                  â”‚
â”‚  â€¢ Model selection           â”‚  â€¢ Send button (â¤)                 â”‚
â”‚  â€¢ Initialize button         â”‚                                    â”‚
â”‚  â€¢ Status indicator          â”‚                                    â”‚
â”‚                               â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Types and Examples

#### 1. Exploratory Questions

**Goal**: Understand what data is available

**Examples**:

```
Q: "What data do you have?"
A: Lists all tables with descriptions and row counts

Q: "What columns are in the sales table?"
A: Lists all columns with their types and meanings

Q: "How many records are in the database?"
A: Provides row counts for all tables with total
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: What data is available?                           â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ I have access to 3 tables in your database:            â”‚
â”‚                                                         â”‚
â”‚ 1. **sales_transactions** (10,000 rows)                â”‚
â”‚    Transaction records with sales data including:      â”‚
â”‚    â€¢ Transaction ID and date                           â”‚
â”‚    â€¢ Sale amounts                                      â”‚
â”‚    â€¢ Product categories                                â”‚
â”‚    â€¢ Customer information                              â”‚
â”‚    â€¢ Payment methods                                   â”‚
â”‚                                                         â”‚
â”‚ 2. **products** (500 rows)                             â”‚
â”‚    Product catalog with details like:                  â”‚
â”‚    â€¢ Product names and IDs                             â”‚
â”‚    â€¢ Categories and prices                             â”‚
â”‚    â€¢ Stock quantities                                  â”‚
â”‚                                                         â”‚
â”‚ 3. **customers** (2,500 rows)                          â”‚
â”‚    Customer records containing:                        â”‚
â”‚    â€¢ Customer IDs and names                            â”‚
â”‚    â€¢ Contact information                               â”‚
â”‚    â€¢ Join dates                                        â”‚
â”‚    â€¢ Loyalty tiers                                     â”‚
â”‚                                                         â”‚
â”‚ What would you like to explore?                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Simple Aggregation Queries

**Goal**: Get summary statistics

**Examples**:

```
Q: "What is the total sales amount?"
Q: "How many transactions were there last month?"
Q: "What's the average order value?"
Q: "Show me sales by category"
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: What is the total sales amount?                   â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ The total sales amount across all transactions is      â”‚
â”‚ **$4,523,876.45**.                                     â”‚
â”‚                                                         â”‚
â”‚ This represents 10,000 transactions with an average    â”‚
â”‚ order value of $452.39.                                â”‚
â”‚                                                         â”‚
â”‚ Breakdown by category:                                 â”‚
â”‚ â€¢ Electronics: $1,234,567.89 (27.3%)                   â”‚
â”‚ â€¢ Home: $987,654.32 (21.8%)                            â”‚
â”‚ â€¢ Fashion: $876,543.21 (19.4%)                         â”‚
â”‚ â€¢ Sports: $765,432.10 (16.9%)                          â”‚
â”‚ â€¢ Beauty: $659,678.93 (14.6%)                          â”‚
â”‚                                                         â”‚
â”‚ [Data Table with detailed breakdown]                   â”‚
â”‚   category      total_sales    percentage              â”‚
â”‚ 0 Electronics   1234567.89     27.3%                   â”‚
â”‚ 1 Home           987654.32     21.8%                   â”‚
â”‚ 2 Fashion        876543.21     19.4%                   â”‚
â”‚ 3 Sports         765432.10     16.9%                   â”‚
â”‚ 4 Beauty         659678.93     14.6%                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. Top N / Bottom N Queries

**Goal**: Find extremes in data

**Examples**:

```
Q: "Show me the top 10 customers by spending"
Q: "What are the 5 least selling products?"
Q: "Find the highest value transactions"
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: Show me the top 5 customers by total spending    â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ Here are your top 5 customers by total spending:       â”‚
â”‚                                                         â”‚
â”‚ 1. **Sarah Johnson** (CUST-1023)                       â”‚
â”‚    â€¢ Total: $15,678.45                                 â”‚
â”‚    â€¢ Transactions: 47                                  â”‚
â”‚    â€¢ Avg Order: $333.58                                â”‚
â”‚    â€¢ Loyalty: Platinum                                 â”‚
â”‚                                                         â”‚
â”‚ 2. **Michael Chen** (CUST-2156)                        â”‚
â”‚    â€¢ Total: $14,234.12                                 â”‚
â”‚    â€¢ Transactions: 38                                  â”‚
â”‚    â€¢ Avg Order: $374.58                                â”‚
â”‚    â€¢ Loyalty: Platinum                                 â”‚
â”‚                                                         â”‚
â”‚ 3. **Emily Rodriguez** (CUST-0789)                     â”‚
â”‚    â€¢ Total: $12,987.33                                 â”‚
â”‚    â€¢ Transactions: 41                                  â”‚
â”‚    â€¢ Avg Order: $316.76                                â”‚
â”‚    â€¢ Loyalty: Gold                                     â”‚
â”‚                                                         â”‚
â”‚ 4. **David Kim** (CUST-1445)                           â”‚
â”‚    â€¢ Total: $11,543.67                                 â”‚
â”‚    â€¢ Transactions: 35                                  â”‚
â”‚    â€¢ Avg Order: $329.82                                â”‚
â”‚    â€¢ Loyalty: Gold                                     â”‚
â”‚                                                         â”‚
â”‚ 5. **Amanda White** (CUST-0334)                        â”‚
â”‚    â€¢ Total: $10,876.54                                 â”‚
â”‚    â€¢ Transactions: 29                                  â”‚
â”‚    â€¢ Avg Order: $375.05                                â”‚
â”‚    â€¢ Loyalty: Gold                                     â”‚
â”‚                                                         â”‚
â”‚ These 5 customers account for $65,319.11 (1.4% of      â”‚
â”‚ total revenue). Consider VIP treatment programs!       â”‚
â”‚                                                         â”‚
â”‚ [Data Table with full details]                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4. Visualization Requests

**Goal**: See data as charts

**Visualization Keywords**: chart, graph, plot, visualize, show as, pie, bar, line, scatter

**Examples**:

```
Q: "Show sales by month as a line chart"
Q: "Visualize category distribution as a pie chart"
Q: "Create a bar chart of top products"
Q: "Plot sales trend over time"
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: Show me monthly sales as a line chart             â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ Here's the monthly sales trend for 2024:               â”‚
â”‚                                                         â”‚
â”‚ [Interactive Plotly Line Chart]                         â”‚
â”‚                                                         â”‚
â”‚  $600K â”‚                           â•±â”€â”€â”€â”€â”€â•²             â”‚
â”‚        â”‚                         â•±         â•²           â”‚
â”‚  $500K â”‚                       â•±             â•²         â”‚
â”‚        â”‚                     â•±                 â•²       â”‚
â”‚  $400K â”‚           â•±â”€â”€â”€â”€â”€â”€â”€â•±                     â•²     â”‚
â”‚        â”‚         â•±                                 â•²   â”‚
â”‚  $300K â”‚       â•±                                     â•² â”‚
â”‚        â”‚     â•±                                         â”‚
â”‚  $200K â”‚   â•±                                           â”‚
â”‚        â”‚ â•±                                             â”‚
â”‚  $100K â”‚                                               â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚         Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Decâ”‚
â”‚                                                         â”‚
â”‚ **Key Insights**:                                      â”‚
â”‚ â€¢ Steady growth from Jan ($250K) to Jul ($580K)        â”‚
â”‚ â€¢ Peak sales in July coinciding with summer season     â”‚
â”‚ â€¢ Slight dip in Aug-Sep, recovered in Oct              â”‚
â”‚ â€¢ Year-over-year growth: +34%                          â”‚
â”‚ â€¢ Best performing months: Jul, Jun, Oct                â”‚
â”‚                                                         â”‚
â”‚ Hover over the chart for exact values!                 â”‚
â”‚                                                         â”‚
â”‚ [Data Table]                                            â”‚
â”‚   month    total_sales    growth_pct                   â”‚
â”‚ 0 Jan      250,234.56     --                           â”‚
â”‚ 1 Feb      278,567.89     +11.3%                       â”‚
â”‚ 2 Mar      312,456.78     +12.2%                       â”‚
â”‚ ...                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5. Multi-Turn Conversations (Context-Aware)

**Goal**: Build on previous answers

**Examples**:

```
# Turn 1
Q: "Show me sales by category"
A: [Table with categories]

# Turn 2 (references "those")
Q: "Show those as a bar chart"
A: [Bar chart with same data, no new SQL query]

# Turn 3 (references "that")
Q: "Sort that by highest to lowest"
A: [Re-sorted chart]

# Turn 4 (transformation)
Q: "Only show categories over $500K"
A: [Filtered data]
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: What are the total sales by category?             â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ Here are the total sales by category:                  â”‚
â”‚                                                         â”‚
â”‚ [Data Table]                                            â”‚
â”‚   category      total_sales                             â”‚
â”‚ 0 Electronics   1,234,567.89                            â”‚
â”‚ 1 Home            987,654.32                            â”‚
â”‚ 2 Fashion         876,543.21                            â”‚
â”‚ 3 Sports          765,432.10                            â”‚
â”‚ 4 Beauty          659,678.93                            â”‚
â”‚                                                         â”‚
â”‚ Total across all categories: $4,523,876.45              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User: Show those as a bar chart                         â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ I've created a bar chart for the sales by category:    â”‚
â”‚                                                         â”‚
â”‚ [Interactive Plotly Bar Chart]                          â”‚
â”‚                                                         â”‚
â”‚  $1.4M â”‚                                                â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ                                        â”‚
â”‚  $1.2M â”‚     â–ˆâ–ˆâ–ˆ                                        â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ                                        â”‚
â”‚  $1.0M â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                                â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                                â”‚
â”‚  $800K â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                        â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                â”‚
â”‚  $600K â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ        â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ        â”‚
â”‚  $400K â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ        â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ        â”‚
â”‚  $200K â”‚     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ        â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚         Elec    Home    Fash    Sport   Beau           â”‚
â”‚                                                         â”‚
â”‚ [Same Data Table]                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User: Only show categories above $800K                  â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ Filtered to show only categories with sales over $800K:â”‚
â”‚                                                         â”‚
â”‚ [Updated Bar Chart with 3 bars]                         â”‚
â”‚                                                         â”‚
â”‚ [Filtered Data Table]                                   â”‚
â”‚   category      total_sales                             â”‚
â”‚ 0 Electronics   1,234,567.89                            â”‚
â”‚ 1 Home            987,654.32                            â”‚
â”‚ 2 Fashion         876,543.21                            â”‚
â”‚                                                         â”‚
â”‚ These 3 categories represent 70% of total revenue.      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User: What percentage does Electronics represent?       â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ Electronics represents **27.3%** of total sales.        â”‚
â”‚                                                         â”‚
â”‚ Calculation:                                            â”‚
â”‚ â€¢ Electronics: $1,234,567.89                            â”‚
â”‚ â€¢ Total Sales: $4,523,876.45                            â”‚
â”‚ â€¢ Percentage: 1,234,567.89 Ã· 4,523,876.45 = 0.273       â”‚
â”‚                                                         â”‚
â”‚ This makes Electronics your #1 category, outperforming  â”‚
â”‚ the second-place Home category by 5.5 percentage points.â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What Happened Behind the Scenes**:
1. **Turn 1**: New SQL query generated
2. **Turn 2**: Detected "those" â†’ reused DataFrame, created visualization (faster)
3. **Turn 3**: Detected "only show" â†’ generated pandas filter code
4. **Turn 4**: Detected "percentage" â†’ calculated from existing data in memory

#### 6. Comparative Analysis

**Goal**: Compare different segments

**Examples**:

```
Q: "Compare Q1 vs Q2 sales"
Q: "Which category grew the most year-over-year?"
Q: "Show the difference between weekday and weekend sales"
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: Compare sales between Q1 and Q2 of 2024           â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ **Q1 vs Q2 Sales Comparison (2024)**                   â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Overall Performance:                                â”‚
â”‚ â€¢ Q1 (Jan-Mar): $1,234,567.00                          â”‚
â”‚ â€¢ Q2 (Apr-Jun): $1,456,789.00                          â”‚
â”‚ â€¢ Difference: +$222,222.00 (+18.0%)                    â”‚
â”‚                                                         â”‚
â”‚ ğŸ“ˆ Growth by Category:                                 â”‚
â”‚                                                         â”‚
â”‚ [Grouped Bar Chart]                                     â”‚
â”‚                                                         â”‚
â”‚  $600K â”‚                                                â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ                                        â”‚
â”‚  $500K â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ                                    â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                            â”‚
â”‚  $400K â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ                        â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                â”‚
â”‚  $300K â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ            â”‚
â”‚        â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â”‚
â”‚  $200K â”‚     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚         Elec        Home        Fash        Sport  Beau â”‚
â”‚         â–ˆ Q1  â–ˆ Q2                                      â”‚
â”‚                                                         â”‚
â”‚ **Key Findings**:                                      â”‚
â”‚                                                         â”‚
â”‚ ğŸ”¥ Strongest Growth:                                   â”‚
â”‚ 1. Electronics: +45% ($280K â†’ $406K)                   â”‚
â”‚ 2. Home: +32% ($245K â†’ $323K)                          â”‚
â”‚ 3. Fashion: +15% ($267K â†’ $307K)                       â”‚
â”‚                                                         â”‚
â”‚ ğŸ“‰ Slower Growth:                                      â”‚
â”‚ â€¢ Sports: +8% ($234K â†’ $253K)                          â”‚
â”‚ â€¢ Beauty: +5% ($208K â†’ $218K)                          â”‚
â”‚                                                         â”‚
â”‚ ğŸ’¡ Insights:                                           â”‚
â”‚ â€¢ Electronics momentum driven by new product launches   â”‚
â”‚ â€¢ Home category benefited from seasonal promotions     â”‚
â”‚ â€¢ Consider targeted campaigns for Sports and Beauty    â”‚
â”‚   to boost Q3 performance                              â”‚
â”‚                                                         â”‚
â”‚ [Detailed Data Table]                                   â”‚
â”‚   category      q1_sales    q2_sales    growth_pct     â”‚
â”‚ 0 Electronics   280,123.45  406,234.56  +45.0%         â”‚
â”‚ 1 Home          245,678.90  323,456.78  +31.7%         â”‚
â”‚ 2 Fashion       267,345.12  307,123.89  +14.9%         â”‚
â”‚ 3 Sports        233,567.89  252,678.34  +8.2%          â”‚
â”‚ 4 Beauty        207,851.64  217,295.43  +4.5%          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 7. Time-Based Analysis

**Goal**: Understand trends over time

**Examples**:

```
Q: "Show daily sales for the last 30 days"
Q: "What day of week has highest sales?"
Q: "How do sales vary by hour?"
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: What day of the week has the highest sales?       â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ **Sales by Day of Week Analysis**                      â”‚
â”‚                                                         â”‚
â”‚ [Bar Chart]                                             â”‚
â”‚                                                         â”‚
â”‚  $800K â”‚                                                â”‚
â”‚        â”‚                         â–ˆâ–ˆâ–ˆ                    â”‚
â”‚  $700K â”‚                         â–ˆâ–ˆâ–ˆ                    â”‚
â”‚        â”‚                 â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ                    â”‚
â”‚  $600K â”‚         â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ            â”‚
â”‚        â”‚         â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ            â”‚
â”‚  $500K â”‚         â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ            â”‚
â”‚        â”‚         â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â”‚
â”‚  $400K â”‚         â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â”‚
â”‚        â”‚   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â”‚
â”‚  $300K â”‚   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚         Mon   Tue   Wed   Thu   Fri   Sat   Sun        â”‚
â”‚                                                         â”‚
â”‚ **Winner: Friday** ğŸ†                                  â”‚
â”‚ â€¢ Total: $782,345.67                                   â”‚
â”‚ â€¢ Avg Order: $487.92                                   â”‚
â”‚ â€¢ Transactions: 1,604                                  â”‚
â”‚ â€¢ 19.2% above weekly average                           â”‚
â”‚                                                         â”‚
â”‚ **Full Breakdown**:                                    â”‚
â”‚ 1. Friday: $782K (22.4%)                               â”‚
â”‚ 2. Saturday: $723K (20.7%)                             â”‚
â”‚ 3. Thursday: $691K (19.8%)                             â”‚
â”‚ 4. Wednesday: $634K (18.2%)                            â”‚
â”‚ 5. Tuesday: $567K (16.2%)                              â”‚
â”‚ 6. Sunday: $489K (14.0%)                               â”‚
â”‚ 7. Monday: $345K (9.9%)                                â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Patterns:                                           â”‚
â”‚ â€¢ Weekend (Fri-Sun): 57.1% of weekly sales             â”‚
â”‚ â€¢ Weekday (Mon-Thu): 42.9% of weekly sales             â”‚
â”‚ â€¢ Monday is slowest (53% below Friday)                 â”‚
â”‚                                                         â”‚
â”‚ ğŸ’¡ Recommendations:                                    â”‚
â”‚ â€¢ Schedule promotions Thursday-Saturday                â”‚
â”‚ â€¢ Consider Monday special offers to boost traffic      â”‚
â”‚ â€¢ Ensure adequate staffing Friday-Sunday               â”‚
â”‚                                                         â”‚
â”‚ [Data Table]                                            â”‚
â”‚   day_of_week   total_sales   avg_order   transactions â”‚
â”‚ 0 Friday        782,345.67    487.92      1,604        â”‚
â”‚ 1 Saturday      723,456.78    451.23      1,604        â”‚
â”‚ ...                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 8. Distribution Analysis

**Goal**: Understand spread and outliers

**Examples**:

```
Q: "Show the distribution of order amounts"
Q: "What's the profit margin distribution by category?"
Q: "Are there any outliers in customer spending?"
```

**Example Interaction**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: Show profit margin distribution by category       â”‚
â”‚      using a box plot                                   â”‚
â”‚                                                         â”‚
â”‚ Assistant:                                              â”‚
â”‚ Here's the profit margin distribution across categories:â”‚
â”‚                                                         â”‚
â”‚ [Interactive Plotly Box Plot]                           â”‚
â”‚                                                         â”‚
â”‚  60% â”‚                                  âš¬               â”‚
â”‚      â”‚                                                  â”‚
â”‚  50% â”‚         â”Œâ”€â”€â”€â”                                    â”‚
â”‚      â”‚         â”‚   â”‚         â”Œâ”€â”€â”€â”                      â”‚
â”‚  40% â”‚    â”Œâ”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”   â”‚   â”‚    â”Œâ”€â”€â”€â”             â”‚
â”‚      â”‚    â”‚    â”‚   â”‚    â”‚   â”‚   â”‚    â”‚   â”‚             â”‚
â”‚  30% â”‚    â”‚    â””â”€â”€â”€â”˜    â”‚   â”‚   â”‚    â”‚   â”‚    â”Œâ”€â”€â”€â”    â”‚
â”‚      â”‚    â”‚      â–ˆ      â”‚   â””â”€â”€â”€â”˜    â”‚   â”‚    â”‚   â”‚    â”‚
â”‚  20% â”‚    â”‚             â”‚     â–ˆ      â””â”€â”€â”€â”˜    â”‚   â”‚    â”‚
â”‚      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â–ˆ       â””â”€â”€â”€â”˜    â”‚
â”‚  10% â”‚                                          â–ˆ       â”‚
â”‚      â”‚              âš¬                                   â”‚
â”‚   0% â”‚                                                  â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚         Elec    Home    Fash    Sport   Beau           â”‚
â”‚                                                         â”‚
â”‚ **Key Statistics**:                                    â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Electronics:                                        â”‚
â”‚ â€¢ Median: 32.5%                                        â”‚
â”‚ â€¢ Range: 15.2% to 55.8%                                â”‚
â”‚ â€¢ IQR: 28.1% - 38.9%                                   â”‚
â”‚ â€¢ Outliers: 2 high (>55%)                              â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Home:                                               â”‚
â”‚ â€¢ Median: 28.7%                                        â”‚
â”‚ â€¢ Range: 12.3% to 48.9%                                â”‚
â”‚ â€¢ IQR: 24.5% - 34.2%                                   â”‚
â”‚ â€¢ Consistent profitability                             â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Fashion:                                            â”‚
â”‚ â€¢ Median: 35.2%                                        â”‚
â”‚ â€¢ Range: 18.7% to 52.1%                                â”‚
â”‚ â€¢ IQR: 30.1% - 41.3%                                   â”‚
â”‚ â€¢ Highest median margin                                â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Sports:                                             â”‚
â”‚ â€¢ Median: 22.3%                                        â”‚
â”‚ â€¢ Range: 8.9% to 38.7%                                 â”‚
â”‚ â€¢ IQR: 18.4% - 28.1%                                   â”‚
â”‚ â€¢ Lower margins, competitive pricing                   â”‚
â”‚                                                         â”‚
â”‚ ğŸ“Š Beauty:                                             â”‚
â”‚ â€¢ Median: 18.5%                                        â”‚
â”‚ â€¢ Range: 5.2% to 32.4%                                 â”‚
â”‚ â€¢ IQR: 14.7% - 23.8%                                   â”‚
â”‚ â€¢ Lowest margins, review pricing strategy              â”‚
â”‚                                                         â”‚
â”‚ ğŸ’¡ Insights:                                           â”‚
â”‚ â€¢ Fashion offers best profit margins (35.2% median)    â”‚
â”‚ â€¢ Beauty needs pricing optimization (18.5% median)     â”‚
â”‚ â€¢ Electronics shows high variability (outliers exist)  â”‚
â”‚ â€¢ Consider focusing on high-margin Fashion items       â”‚
â”‚                                                         â”‚
â”‚ [Detailed Statistics Table]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Advanced Features

#### Conversation Management

**Save and Resume Conversations**:

1. **Auto-Save**: Every interaction is automatically saved
   - Location: `conversations/<uuid>.json`
   - Includes: All messages, data, visualizations

2. **Load Previous Conversation**:
   - Click conversation in sidebar
   - Entire history restored
   - Can continue asking questions

3. **New Conversation**:
   - Click "+ New Chat"
   - Starts fresh (previous data not accessible)
   - Previous conversation remains saved

4. **Delete Conversation**:
   - Select conversation
   - Click ğŸ—‘ï¸ button
   - Permanently removed

**Example**:
```
# Session 1 (Monday)
Q: "Show me last week's sales"
[Get answer and close browser]

# Session 2 (Tuesday)
[Open browser, select Monday's conversation from sidebar]
Q: "How does that compare to the week before?"
[System knows "that" refers to last week's data]
```

#### Data Transformation

**Transform previous results without re-querying**:

```
# Get initial data
Q: "Show all sales from last month"
[Returns 1,000 rows]

# Transform without new SQL
Q: "Filter only amounts over $500"
[Uses pandas to filter existing DataFrame]

Q: "Sort by date descending"
[Sorts existing data]

Q: "Group by category and sum amounts"
[Aggregates existing data]
```

**How It Works**:
- System detects transformation intent
- LLM generates pandas code
- Code executed safely on existing DataFrame
- Much faster than new SQL query

#### Export and Share

**Export Visualizations**:
```python
# Charts are interactive Plotly figures
# Right-click on chart â†’ Download as PNG/SVG/PDF
# Or use built-in Plotly controls:
# ğŸ“· Camera icon â†’ Save as PNG
# ğŸ” Zoom, pan, reset tools available
```

**Export Data**:
```python
# Data tables displayed as HTML
# Can be copied and pasted into Excel
# Or right-click â†’ Copy
```

### Troubleshooting Common Issues

#### Issue 1: "Agent not responding"

**Symptoms**: Spinning indicator, no answer

**Causes**:
1. Ollama not running
2. Model not loaded
3. Database connection lost

**Solutions**:
```powershell
# Check Ollama status
curl http://localhost:11434/api/tags

# Restart Ollama if needed
ollama serve

# Reload page and reinitialize agent
```

#### Issue 2: "SQL execution error"

**Symptoms**: Error message in chat

**Causes**:
1. Invalid table/column reference
2. Syntax error in generated SQL
3. Database locked

**Solutions**:
- **Automatic**: System retries with error feedback to LLM
- **Manual**: Rephrase question with explicit table/column names
- **Example**: Instead of "Show sales", try "Show data from sales_transactions table"

#### Issue 3: "No visualization created"

**Symptoms**: Answer but no chart

**Causes**:
1. No viz keywords in question
2. Data not suitable for visualization
3. Chart creation failed

**Solutions**:
```
# Be explicit about visualization
âŒ "Show me sales data"
âœ… "Show me sales data as a bar chart"

# Or ask for viz after getting data
Q: "Show me sales by category"
[Get table]
Q: "Visualize that as a bar chart"
[Get chart]
```

#### Issue 4: "Can't find previous data"

**Symptoms**: "Show me those" doesn't work

**Causes**:
1. Started new conversation (data not persisted across chats)
2. Too many turns (data contexts auto-cleaned after 10)

**Solutions**:
- Use same conversation
- Re-run original query if needed
- Loaded conversations restore all data

#### Issue 5: "Slow responses"

**Symptoms**: Takes >10 seconds per answer

**Causes**:
1. Large database (many tables/columns)
2. Complex query
3. System resources limited

**Solutions**:
```powershell
# Use smaller, faster model (if accuracy acceptable)
ollama pull qwen2.5:3b  # Half the size of 7b

# Or optimize ChromaDB
# Reduce metadata verbosity
# Limit semantic search results

# Or increase system resources
# Close other applications
# Use GPU if available
```

### Best Practices for Getting Good Answers

#### 1. Be Specific

```
âŒ "Show me data"
âœ… "Show me the top 10 sales transactions by amount"

âŒ "What about last month?"
âœ… "What were the total sales for March 2024?"
```

#### 2. Use Visualization Keywords

```
âœ… "Show me sales by category as a bar chart"
âœ… "Visualize the trend"
âœ… "Create a pie chart of distribution"
âœ… "Plot the correlation"
```

#### 3. Build on Previous Questions

```
Q1: "What are the total sales by region?"
Q2: "Show those as a map"  # References Q1 results
Q3: "Which region grew the most?"  # Still in context
Q4: "Show me the top customers in that region"  # Still connected
```

#### 4. Provide Context When Needed

```
âœ… "Show me sales for Q1 2024"  # Clear time period
âœ… "Compare Electronics vs Home categories"  # Clear comparison
âœ… "Filter transactions above $1000"  # Clear threshold
```

#### 5. Ask for Explanations

```
Q: "Why is Electronics the best category?"
Q: "What factors contributed to July's peak?"
Q: "Explain the correlation between price and quantity"
```

### Summary: Getting From Zero to Answer

**Complete Flow**:

```
1. âœ… Install Python 3.10+ and Ollama
2. âœ… Pull models (qwen2.5:7b, nomic-embed-text)
3. âœ… Setup project and install dependencies
4. âœ… Prepare database (existing or convert from CSV)
5. âœ… Run analyzer to create vector database
6. âœ… Launch Gradio UI
7. âœ… Initialize agent in browser
8. âœ… Ask question in natural language
9. âœ… Get answer with optional visualization
10. âœ… Continue conversation with follow-ups
```

**Time Investment**:
- Initial setup: 20-30 minutes
- Database analysis: 5-10 minutes
- Ongoing usage: 3-6 seconds per query

**Result**:
- Natural language interface to your data
- No SQL knowledge required
- Intelligent visualizations
- Conversation memory
- Production-ready system

---

## CONCLUSION

This system transforms complex data analysis into natural conversation. By combining:
- **LLM intelligence** (qwen2.5:7b) for understanding and generation
- **Vector database** (ChromaDB) for semantic metadata search
- **Context management** (conversation state) for multi-turn awareness
- **Automatic visualization** (Plotly) for insights
- **Web interface** (Gradio) for accessibility

You get a powerful analytics assistant that:
- âœ… Understands natural language questions
- âœ… Generates accurate SQL automatically
- âœ… Provides insightful answers
- âœ… Creates beautiful visualizations
- âœ… Remembers conversation context
- âœ… Requires no technical knowledge to use

**Start asking questions and let the AI do the heavy lifting!** ğŸš€
